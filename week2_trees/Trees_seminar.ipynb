{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore and benchmark different tree models (that is Decision Trees, Random Forest and Gradient Boosting) using a (toy) binary classification as a playground.   \n",
    "Credit: this notebook was inspired by and built upon [this notebook](https://github.com/yandexdataschool/mlhep2019/blob/master/notebooks/day-2/02_decision_trees_and_ensembles.ipynb) from [MLHEP 2019 school](https://indico.cern.ch/event/768915/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but firstly, do you remember how decision tree is constructed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **generate data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly generate some toy dataset with 2 features (this would make our studies very easy to visualize) and 2 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_toy, y_toy = make_blobs(n_samples=400,\n",
    "                          centers=[[0., 1.], [1., 2.]],\n",
    "                          random_state=42)\n",
    "\n",
    "plt.scatter(X_toy[:, 0], X_toy[:, 1], c=y_toy, alpha=0.8, cmap='bwr')\n",
    "plt.xlabel('X1'), plt.ylabel('X2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def plot_decision_boundary(clf,\n",
    "                          X: np.ndarray,\n",
    "                          y: np.ndarray,\n",
    "                          grid_step: float=0.02,\n",
    "                          cmap='bwr',\n",
    "                          alpha:float=0.6,\n",
    "                          axes=None\n",
    "        ):\n",
    "    \"\"\"\n",
    "    Plot the decision boundary of a classifier, visualize selected points\n",
    "    Args:\n",
    "      clf: a fitted model, must support predict method\n",
    "      X[n_examples, n_features]: points where to evaluate the classifier\n",
    "      y[n_examples]: true labels\n",
    "      grid_step: decision boundary plottting grid\n",
    "      alpha: opacity of the decision boundary\n",
    "      axes(matplotlib.axes._subplots.AxesSubplot): axes where plot, if None, a new figure is created\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the grid\n",
    "    x_top_left = X.min(axis=0) - 1\n",
    "    x_bottom_right = X.max(axis=0) + 1\n",
    "    grid_x0, grid_x1 = np.meshgrid(\n",
    "         np.arange(x_top_left[0], x_bottom_right[0], grid_step),\n",
    "         np.arange(x_top_left[1], x_bottom_right[1], grid_step)\n",
    "      )\n",
    "    \n",
    "    # Calculate predictions on the grid\n",
    "    y_pred_grid = clf.predict(\n",
    "                        np.stack(\n",
    "                              [\n",
    "                                grid_x0.ravel(),\n",
    "                                grid_x1.ravel()\n",
    "                              ],\n",
    "                              axis=1\n",
    "                            )\n",
    "                      ).reshape(grid_x1.shape)\n",
    "    \n",
    "    # Find optimal contour levels and make a filled\n",
    "    # contour plot of predictions\n",
    "    labels = np.sort(np.unique(y))\n",
    "    labels = np.concatenate([[labels[0] - 1],\n",
    "                             labels,\n",
    "                             [labels[-1] + 1]])\n",
    "    medians = (labels[1:] + labels[:-1]) / 2\n",
    "    if axes is None:\n",
    "      _, axes = plt.subplots()\n",
    "    axes.contourf(grid_x0, grid_x1, y_pred_grid, cmap=cmap, alpha=alpha,\n",
    "                 levels=medians)\n",
    "    \n",
    "    # Scatter data points on top of the plot,\n",
    "    # with different styles for correct and wrong\n",
    "    # predictions\n",
    "    y_pred = clf.predict(X)\n",
    "    axes.scatter(*X[y_pred==y].T, c=y[y_pred==y],\n",
    "                marker='o', cmap=cmap, s=10, label='correct')\n",
    "    axes.scatter(*X[y_pred!=y].T, c=y[y_pred!=y],\n",
    "                marker='x', cmap=cmap, s=50, label='errors')\n",
    "\n",
    "    # Dummy plot call to print the accuracy in the legend.\n",
    "    axes.plot([], [], ' ',\n",
    "             label='Accuracy = {:.3f}'.format(accuracy_score(y, y_pred)))\n",
    "    axes.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **decision boundary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we start from decision trees, let's import corresponding `DecisionTreeClassifier` class from `sklearn` library and fit it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=3) \n",
    "clf.fit(X_toy, y_toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plot_decision_boundary(clf, X_toy, y_toy, axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trees themselves also can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "plot_tree(clf, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is done, we can predict class labels with a good old `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X_toy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or use `predict_proba()` to return probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: strictly speaking, this output cannot be interpreted as probability of a class\n",
    "# a calibration procedure is supposed to eliminate this difference, so please have a look here for some methods:\n",
    "#  https://scikit-learn.org/stable/modules/calibration.html\n",
    "\n",
    "clf.predict_proba(X_toy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now firstl, let's investigate how the decision boundary depends on the tree depth. Maximum tree depth is defined by the `max_depth` parameter.   \n",
    "Try out the following values: ``[1, 2, 3, 5, 10, 100]`` and make decision boundary plots for both train and test datasets (separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_toy_train, X_toy_test, y_toy_train, y_toy_test = \\\n",
    "    train_test_split(X_toy, y_toy, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_values = [1, 2, 3, 5, 10, 100]\n",
    "\n",
    "fig, axes_matrix = plt.subplots(nrows=len(depth_values), ncols=2,\n",
    "                                figsize=(2*5, 5*len(depth_values)))\n",
    "for depth, (axes_train, axes_test) in zip(depth_values, axes_matrix):\n",
    "  ### your code here ###\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we were tuning only `max_depth` parameter, but as you might've remembered from the lecture, there's more of them. For example:\n",
    "\n",
    "* `min_samples_split` – there should be at least this many samples to split further (default: 2)\n",
    "* `min_samples_leaf` – there should be at least this many samples on one side of a split to consider it valid (default: 1)\n",
    "* `max_leaf_nodes` – there can be at most this number of leaves in the tree (default: unlimited)\n",
    "* `criterion` – the function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain (default: 'gini')\n",
    "* `min_impurity_decrease` – node will be split if the split induces a decrease of the impurity greater than or equal to this value (default: 0)\n",
    "\n",
    "Note: you can always look them up in the [DecisionTree API](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) or using `Tab` inside of the class instance declaration.\n",
    "\n",
    "Now try to adjust these parameters to get the largest accuracy on the test set. How far can you reach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune me\n",
    "tree_params = {'max_depth': None,\n",
    "               'min_samples_split': 2,\n",
    "               'min_samples_leaf': 1, \n",
    "               'max_leaf_nodes': None, \n",
    "               'criterion': 'gini',\n",
    "               'min_impurity_decrease': 0,\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(**tree_params)\n",
    "clf.fit(X_toy_train, y_toy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_matrix = plt.subplots(nrows=1, ncols=2, figsize=(2*5, 5))\n",
    "plot_decision_boundary(clf, X_toy_train, y_toy_train, axes=axes_matrix[0])\n",
    "plot_decision_boundary(clf, X_toy_test, y_toy_test, axes=axes_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, once we got familiar with single decision trees, let's combine them together into a (presumably) more powerful Random Forest. Then we fit them to the same toy binary classification data and compare with the single tree results from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but firstly, do you remember how Random Forest is constructed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a dictionary with RF parameters set to default values. Play around them and try to understand how each parameter affects the final result. Do you achieve higher performance with the ensemble comparing to a single decision tree? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune me\n",
    "rf_params = {\n",
    "    \n",
    "    # tree params\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1, \n",
    "    'max_leaf_nodes': None, \n",
    "    'criterion': 'gini',\n",
    "    'min_impurity_decrease': 0,\n",
    "    \n",
    "    # ensemble params\n",
    "    'max_features': 'auto', # number of features to consider when looking for the best split. 'auto' = sqrt(n_features) \n",
    "    'n_estimators': 100,  # number of trees in ensemble\n",
    "    'bootstrap': True, # whether to train each tree on a bootstrapped sample\n",
    "    'oob_score': False, # whether to calculate out-of-bag score\n",
    "    'n_jobs': -1 # number of parallel training processes, -1 means to use all available\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(**rf_params) \n",
    "rf.fit(X_toy_train, y_toy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_matrix = plt.subplots(nrows=1, ncols=2, figsize=(2*5, 5))\n",
    "plot_decision_boundary(rf, X_toy_train, y_toy_train, axes=axes_matrix[0])\n",
    "plot_decision_boundary(rf, X_toy_test, y_toy_test, axes=axes_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, smart ensembling aka gradient boosting. Here for the sake of consistency we will use the one from `sklearn` library, but it is imperative to mention that there are more advanced libraries which can boost your trees. We will briefly describe them in a Bonus chapter below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but firstly, do you remember how Gradient Boosting is performed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, try to change the parameters below and see how the result of prediction changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune me\n",
    "gbt_params = {\n",
    "    \n",
    "    # tree params\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1, \n",
    "    'max_leaf_nodes': None, \n",
    "    'min_impurity_decrease': 0,\n",
    "    \n",
    "    # ensemble params\n",
    "    'loss': 'deviance', # loss function to be optimized; 'deviance' refers to logistic regression for classification with probabilistic outputs\n",
    "    'learning_rate': 0.1, # learning rate shrinks the contribution of each tree by `learning_rate`\n",
    "    'criterion': 'friedman_mse', # function to measure the quality of a split\n",
    "    'max_features': 'auto', # number of features to consider when looking for the best split. 'auto' = sqrt(n_features) \n",
    "    'n_estimators': 100,  # number of trees in ensemble\n",
    "    'subsample': 1.0, # fraction of samples to be used for fitting the individual base \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbt = GradientBoostingClassifier(**gbt_params) \n",
    "gbt.fit(X_toy_train, y_toy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_matrix = plt.subplots(nrows=1, ncols=2, figsize=(2*5, 5))\n",
    "plot_decision_boundary(gbt, X_toy_train, y_toy_train, axes=axes_matrix[0])\n",
    "plot_decision_boundary(gbt, X_toy_test, y_toy_test, axes=axes_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can you conclude which algorithm (Decision Tree, Random Forest, Gradient Boosting) performs better on this toy problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bonus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as we mentioned earlier, there are several libraries available on the market which can perform boosting on trees. It is highly likely that you will use them in the future, so it's important to mentioned them here. It should also be emphasized that there is no \"best\" library - they are all \"best\" in their own way. So the choice depends largely on type of the problem you want to solve and on your tastes/preferences. Currently, these are the **most prominent libraries for boosting** out there, and we encourage you to read their description and documentations to get the general understanding of each advantages:\n",
    "\n",
    "* [**XGBoost**](https://xgboost.readthedocs.io/en/latest/)\n",
    "* [**LightGBM**](https://lightgbm.readthedocs.io/en/latest/)\n",
    "* [**CatBoost**](https://catboost.ai/)\n",
    "\n",
    "Here we will do **a very simple and dummy benchmarking** of them - no hyperparameter tuning, just fitting models in a regression setting with their libraries' default parameters using some toy data. However, we highly encourage you to do the same on a more complicated dataset and with a more elaborated hyperparameters tuning. For example, [this data](https://www.kaggle.com/c/higgs-boson/data) from Higgs Kaggle challenge is a good place to start, or also it would be interesting to try them out in homework of this module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # to reproduce the same random state\n",
    "train_size = 20 # number of points to train on\n",
    "\n",
    "x_train = np.sort(np.random.uniform(-2., 2., size=train_size))\n",
    "y_train = x_train**3 - 1.5 * x_train**2 - 2.*x_train - 1\n",
    "y_train += np.random.normal(scale=0.7, size=train_size) # adding some noize to the data\n",
    "x_train = x_train.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test_size = 50 \n",
    "\n",
    "# also generate test data\n",
    "x_test = np.sort(np.random.uniform(-2., 2., size=test_size))\n",
    "y_test = x_test**3 - 1.5 * x_test**2 - 2.*x_test - 1\n",
    "y_test += np.random.normal(scale=0.7, size=test_size) \n",
    "x_test = x_test.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train, s=30, color='gray', label='train')\n",
    "plt.scatter(x_test, y_test, s=30, color='black', label='test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as ctb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(x_values, y_true_values, y_pred_values, ax, label, color=None):\n",
    "  \"\"\"\n",
    "  Function that plots the data points and\n",
    "  model prediction\n",
    "  \"\"\"\n",
    "  ax.plot(x_values, y_true_values, 'o', label='true', markersize=6, color='gray')\n",
    "  ax.plot(x_values, y_pred_values, '-', label=label, color=color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_lgb = lgb.LGBMRegressor(n_estimators=5)\n",
    "regressor_xgb = xgb.XGBRegressor(n_estimators=5)\n",
    "regressor_ctb = ctb.CatBoostRegressor(n_estimators=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_lgb.fit(x_train, y_train)\n",
    "regressor_xgb.fit(x_train, y_train)\n",
    "regressor_ctb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_lgb = regressor_lgb.predict(x_train)\n",
    "y_train_preds_xgb = regressor_xgb.predict(x_train)\n",
    "y_train_preds_ctb = regressor_ctb.predict(x_train)\n",
    "\n",
    "y_test_preds_lgb = regressor_lgb.predict(x_test)\n",
    "y_test_preds_xgb = regressor_xgb.predict(x_test)\n",
    "y_test_preds_ctb = regressor_ctb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes_matrix = plt.subplots(nrows=1, ncols=2,\n",
    "                                figsize=(2*5, 5))\n",
    "\n",
    "\n",
    "plot_predictions(x_train, y_train, y_train_preds_lgb, label='LightGBM', ax=axes_matrix[0])\n",
    "plot_predictions(x_train, y_train, y_train_preds_xgb, label='XGBoost', ax=axes_matrix[0])\n",
    "plot_predictions(x_train, y_train, y_train_preds_ctb, label='CatBoost', ax=axes_matrix[0])\n",
    "axes_matrix[0].legend()\n",
    "axes_matrix[0].grid()\n",
    "axes_matrix[0].set_title('train')\n",
    "\n",
    "plot_predictions(x_test, y_test, y_test_preds_lgb, label='LightGBM', ax=axes_matrix[1])\n",
    "plot_predictions(x_test, y_test, y_test_preds_xgb, label='XGBoost', ax=axes_matrix[1])\n",
    "plot_predictions(x_test, y_test, y_test_preds_ctb, label='CatBoost', ax=axes_matrix[1])\n",
    "axes_matrix[1].legend()\n",
    "axes_matrix[1].grid()\n",
    "axes_matrix[1].set_title('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lpi",
   "language": "python",
   "name": "ml-lpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
