{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Particle identification for the hadronic calorimeter imaging prototype**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CALICE AHCAL prototype**\n",
    "\n",
    "The prototype has been installed in the test beam for data taking at the CERN SPS. During two periods in May and in June 2018, \n",
    "events with muon tracks, as well as electron and pion showers in the energy ranges 10 – 100 GeV and 10 – 200 GeV, respectively, \n",
    "have been recorded. \n",
    "\n",
    "In this seminar, data corresponding to simulation of [a highly granular SiPM-on-tile calorimeter prototype](https://arxiv.org/pdf/1808.09281.pdf) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**\n",
    "38 active layers equipped with 24x24 scindilator plates ${30x30x2 mm^3}$ each equipped with individual SiPM readout. \n",
    "\n",
    "<img src=\"images/setup_1.jpg\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Active layers are inserted in between ${15 mm}$ steel plates:\n",
    "\n",
    "<img src=\"images/setup_2.jpg\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Particle identification**\n",
    "\n",
    "<img src=\"images/pid.jpg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants to investigate detector response on particles with given properties, event selection is needed. The goal of the seminar is to construct such event selection model for 10GeV particles, which would be able to classify them according to their type using (almost) raw simulated hits in calorimeter layers.\n",
    "\n",
    "We will use Boosted Decision Tree model for that purpose and specifically [LightGBM](https://lightgbm.readthedocs.io/en/latest/) implementation of it as an example. However, you might want to try other other popular boosting libraries: [XGBoost](https://xgboost.readthedocs.io/en/latest/) and [CatBoost](https://catboost.ai/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data structure**\n",
    "In the dataset we are going to use each entry corresponds to a single event, while each subentry represents one single hit. Hit positions `ahc\\_hitX/Y/Z` in ${mm}$  - Z-axis is along beam direction - and corresponding energy deposition `ahc\\_hitEnergy` in units of Minimum Ionising Particle (MIP) $\\approx 0.5 \\, \\text{MeV}$ are also stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading simulated data from the ROOT files to the Pandas dataframe\n",
    "data = {}\n",
    "\n",
    "variables = ['ahc_hitX','ahc_hitY','ahc_hitZ','ahc_hitEnergy','ahc_st']\n",
    "data['Electron'] = uproot.open(\"data/Reco_Run60973_e-_10GeV_QGSP_BERT_HP_AllEvents.root\")[\"bigtree\"].pandas.df(variables) # ~10k events\n",
    "data['Pion']  = uproot.open(\"data/Reco_Run60973_pi-_10GeV_QGSP_BERT_HP_AllEvents.root\")[\"bigtree\"].pandas.df(variables) # ~10k events\n",
    "data['Muon']  = uproot.open(\"data/Reco_Run60973_mu-_10GeV_QGSP_BERT_HP_AllEvents.root\")[\"bigtree\"].pandas.df(variables) # ~10k events\n",
    "\n",
    "data['Electron']['ID'] = 0 # Labels for the BDT model (chosen arbitrary)\n",
    "data['Pion']['ID'] = 1\n",
    "data['Muon']['ID'] = 2\n",
    "\n",
    "particletypes = ['Electron', 'Pion', 'Muon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot several events (at least 5) for different particle types, explain the difference\n",
    "* How would you describe this visible difference quantatively? Feel free to modify the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evt = 0\n",
    "fig = plt.figure(figsize=[60,15])\n",
    "for k in range(len(particletypes)):\n",
    "    ax = fig.add_subplot(1, 3, k+1, projection='3d')\n",
    "    ax.set_xlim3d(-300, 300)\n",
    "    ax.set_ylim3d(-300, 300)\n",
    "    ax.set_zlim3d(0,1000)\n",
    "    ax.set_title(particletypes[k], fontsize=30)\n",
    "    im = ax.scatter(data[particletypes[k]].loc[evt].ahc_hitX, data[particletypes[k]].loc[evt].ahc_hitY, data[particletypes[k]].loc[evt].ahc_hitZ, \\\n",
    "                  s=40, c=data[particletypes[k]].loc[evt].ahc_hitEnergy, depthshade=True,  cmap='viridis', vmin=0, vmax=10)\n",
    "    fig.colorbar(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or interactive mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "evt = 1\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    specs=[[{'type': 'scene'}, {'type': 'scene'},  {'type': 'scene'}],\n",
    "                          ],\n",
    "                    subplot_titles=particletypes,\n",
    "                    horizontal_spacing = 0.15\n",
    "                   )\n",
    "\n",
    "for i, particle_type in enumerate(particletypes, start=1):\n",
    "    x = data[particle_type].loc[evt].ahc_hitX\n",
    "    y = data[particle_type].loc[evt].ahc_hitY\n",
    "    z = data[particle_type].loc[evt].ahc_hitZ\n",
    "    e = data[particle_type].loc[evt].ahc_hitEnergy\n",
    "    colorbar = dict(title = 'Hit energy deposit,\\n [MIP]', len=0.75,) # x = ..., y = ...\n",
    "    showscale = [False, False, True]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "        name=particle_type,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "#         labels=dict(x=\"x, mm\", y=\"y, mm\", z=\"z, mm\"),\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            color=e,   \n",
    "            cmin=0, cmax=20,\n",
    "            colorscale='Viridis',  \n",
    "            colorbar = colorbar,        \n",
    "            opacity=1.,\n",
    "            showscale=showscale[i-1]\n",
    "        ),   \n",
    "    ),\n",
    "        row=1, col=i)\n",
    "\n",
    "scenedict = dict(\n",
    "            xaxis = dict(nticks=10, range=[-300,300], title = 'x, mm'),\n",
    "            yaxis = dict(nticks=10, range=[-300,300], title = 'y, mm'),\n",
    "            zaxis = dict(nticks=10, range=[0,1000], title = 'z, mm'),\n",
    "            aspectratio=dict(x=0.6, y=0.6, z=1),\n",
    "                    )\n",
    "    \n",
    "fig.update_layout(\n",
    "        scene1 = scenedict,\n",
    "        scene2 = scenedict,\n",
    "        scene3 = scenedict,\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='x, mm')\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text='Event display',\n",
    "    height=800,\n",
    "    width=1600,\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Some theory (in a very nutshell)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Energy depositions of electromagnetic and hadronic showers are described by material-dependent parameters - so called [radiation- and nuclear interaction lengths](http://www.physics.rutgers.edu/~evahal/talks/tasi09/TASI_day3_school.pdf).  \n",
    "\n",
    "In case of steel absorber, radiation lenth ${X_0}$ is ${O(1)}$ is less than nuclear interaction length ${\\lambda_n}$ which, for example, leads to the fact that electromagnetic showers in the AHCAL start to shower much earlier than hadronic ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you draw any other conclusions from this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shower start finding algorithm of CALICE software returns interger number of layer `ahc_st` where start of a shower is observed. Shower start variable was calculated for all events, in case of event that doesn't satisfy method criteria (shower was not found), shower start variable is set to 100. `ahc_st` is already read from the simulation ROOT file and written in Pandas DF and ready to be used in the event selection. Let's check how it looks for our events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for particle in particletypes:\n",
    "    plt.hist(data[particle].groupby(level=0).ahc_st.mean(), range=[0,100], bins=100, log=True, alpha=0.5, label=particle)\n",
    "fig1 = plt.gcf()\n",
    "plt.title(\"Shower start\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see below, one observable is actually already quite enough to build a decent BDT model. Let's preprocess data a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change event indices to avoid merging of events with same index\n",
    "data['Pion'].index.set_levels(data['Pion'].index.levels[0]+10000, \n",
    "                              level=0,\n",
    "                              inplace=True)\n",
    "data['Muon'].index.set_levels(data['Muon'].index.levels[0]+20000, \n",
    "                              level=0,\n",
    "                              inplace=True)\n",
    "\n",
    "data_merged = pd.concat(data).droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Pandas dataframe on event-by-event basis\n",
    "X = pd.DataFrame()\n",
    "\n",
    "X['st'] = data_merged.groupby(level=0).ahc_st.mean()\n",
    "X['ID'] = data_merged.groupby(level=0).ID.mean() \n",
    "features = ['st']\n",
    "y = data_merged.groupby(level=0).ID.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test samples\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.5, random_state=42) #50% fraction may be useful to plot distributions for selected events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data into LightGMB format\n",
    "lgb_train = lgb.Dataset(X_train[features], y_train)\n",
    "lgb_eval = lgb.Dataset(X_test[features], y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'softmax',\n",
    "    'metric': {'multi_logloss'},\n",
    "    'num_class': 3,\n",
    "    'max_depth': 10,\n",
    "    'metric_freq': 1,\n",
    "    'num_leaves': 10,\n",
    "    'max_depth' : 10,\n",
    "    'min_child_samples' : 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# Note: training can also be done with conventional fit() method, smth like this:\n",
    "# classifier = lgb.LGBMClassifier(n_estimators=100)\n",
    "# classifier.fit(X_train[features], y_train)\n",
    "\n",
    "classifier = lgb.LGBMClassifier()\n",
    "evals_result = {} \n",
    "classifier._Booster = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                evals_result = evals_result,\n",
    "                early_stopping_rounds=5)\n",
    "gbm = classifier._Booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss function during training\n",
    "ax = lgb.plot_metric(evals_result, metric='multi_logloss')\n",
    "fig1 = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions with the model\n",
    "y_pred = gbm.predict(X_test[features], num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the most interesting ROC curves\n",
    "for i in range(len(particletypes)):\n",
    "    tpr, fpr, tres = roc_curve(y_test == i, y_pred[...,i])\n",
    "    auc = roc_auc_score(y_test == i, y_pred[...,i])\n",
    "    plt.plot(tpr, fpr, label=particletypes[i] + ' vs all \\n AUC='+str(round(auc, 5)))\n",
    "plt.grid()\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.legend(loc='best')\n",
    "fig1 = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Exercise 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for only one variable. Here are some questions: \n",
    "* Can you explain why is it so?\n",
    "* What kind of events confuse such model and cause it to mispredict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Making predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the classifier's output (aka score) in categories, which are defined by the maximum output value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(y_pred, columns=['e_prob', 'pi_prob', 'mu_prob'])\n",
    "\n",
    "df_pred['pred_type'] = np.argmax(y_pred, axis=1)\n",
    "df_pred['pred_prob'] = np.max(y_pred, axis=1)\n",
    "\n",
    "for i, particle_type in enumerate(particletypes):\n",
    "    plt.hist(df_pred.query(f'pred_type == {i}').pred_prob, label=particle_type, histtype='step', bins=100, range=(0.5, 1.), density=True)\n",
    "plt.grid()\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Saving model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model can be saved, e.g in `.txt` format, for the later usage in your analysis group software framework with the corresponding API. \n",
    "Also note that some libraries [can save trees in if-else format](https://catboost.ai/docs/features/export-model-to-python.html), but not for the multiclass case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.save_model(\"Multiclass_10-200GeV_MC_v2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Homework (main part)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we've got the model baseline. There are several ideas how one can improve it: \n",
    "\n",
    "1. Create new features\n",
    "2. Preprocess data in a clever way\n",
    "3. Tune model hyperparameters \n",
    "4. Try other models (e.g. Random Forest, linear models, Neural Nets) \n",
    "\n",
    "So now we suggest you to do exactly that: try to beat our baseline! We are very curious to see which tricks and ideas helped you to achieve better performance:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lpi",
   "language": "python",
   "name": "ml-lpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
