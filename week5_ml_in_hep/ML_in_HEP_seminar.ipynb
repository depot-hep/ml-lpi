{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HEP analysis in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously in this course you've learnt about different ML techniques and how can they be used in HEP analysis. Now the question is how can we integrate all this ML stuff into typical analysis workflow. Today we will disscuss one possible answer - doing the entire analysis with python using __[scikit-hep](https://scikit-hep.org/)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: This seminar's aim is to only provide some introduction into the scikit-hep ecosystem and it by no means provides a complete picture, for more information on topics discussed below you can follow links that will be provided along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seminar is in large part based on the materials from __[PyHEP 2020 workshop](https://indico.cern.ch/event/882824/timetable/#20200713.detailed)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I/O with .root files**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PyROOT - dynamic bindings to ROOT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to work with root files in python many of may be already familiar with is to use PyROOT - dynamic python bindings to ROOT framework and C++. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ROOT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROOT 6.22, a major revision of PyROOT has been released. The new PyROOT has extensive support for modern C++ (it operates on top of __[cppyy](https://cppyy.readthedocs.io)__)\n",
    "\n",
    "Here is an example of how you can access C++ objects from python using this functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = ROOT.std.vector['float']((1, 2, 3))\n",
    "print(\"ROOT.std.vector['float']\", v1)\n",
    "\n",
    "v2 = np.asarray(v1)\n",
    "print('numpy.array', v2)\n",
    "\n",
    "v1[0] = 42\n",
    "print('numpy.array', v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array((1, 2, 3), dtype=np.float32)\n",
    "print('numpy.array', v1)\n",
    "\n",
    "v2 = ROOT.VecOps.AsRVec(v1)\n",
    "print(\"ROOT.RVec['float']\", v2)\n",
    "\n",
    "v1[0] = 42\n",
    "print(\"ROOT.RVec['float']\", v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call usuall  C++ commands under Pyhton enviroment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.gInterpreter.Declare('''\n",
    "float get_element(float* v, unsigned int i) {\n",
    "    return v[i];\n",
    "}\n",
    "''')\n",
    "\n",
    "print('The first element of the numpy.array is', ROOT.get_element(v1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With PyROOT you can use usual ROOT classes as in following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root'\n",
    "path = './data/opendata_muons_skimmed.root'\n",
    "file = ROOT.TFile.Open(path)\n",
    "tree = file.Events\n",
    "tree.Print()\n",
    "# Can iterate over t\n",
    "dimuon_pt = []\n",
    "for i,event in enumerate(tree):\n",
    "    #if i % 1000 == 0:\n",
    "    #    print('Processing event {}'.format(i))\n",
    "    if event.nMuon == 2:\n",
    "        dimuon_pt.append(event.Muon_pt[0] + event.Muon_pt[1])\n",
    "    if i == 100000:\n",
    "        break\n",
    "plt.hist(dimuon_pt, range=(0, 100), bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RDataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modern interface to process datasets in ROOT files (aka TTrees) is [RDataFrame](https://root.cern/doc/master/classROOT_1_1RDataFrame.html). The concept is a computation graph, which is built in a declarative manner, and executes the booked computations as efficient as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ROOT.RDataFrame('Events', path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the dataset for events with two muons and opposite charge. The last line restricts the full dataset to a subset of the in total 66 mio. events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.Filter(\"nMuon == 2\", \"Events with exactly two muons\")\\\n",
    "       .Filter(\"Muon_charge[0] != Muon_charge[1]\", \"Muons with opposite charge\")\\\n",
    "       .Range(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.gInterpreter.Declare(\n",
    "\"\"\"\n",
    "using Vec_t = const ROOT::VecOps::RVec<float>&;\n",
    "float compute_mass(Vec_t pt, Vec_t eta, Vec_t phi, Vec_t mass) {\n",
    "    ROOT::Math::PtEtaPhiMVector p1(pt[0], eta[0], phi[0], mass[0]);\n",
    "    ROOT::Math::PtEtaPhiMVector p2(pt[1], eta[1], phi[1], mass[1]);\n",
    "    return (p1 + p2).mass();\n",
    "}\n",
    "\"\"\")\n",
    "df = df.Define(\"Dimuon_mass\", \"compute_mass(Muon_pt, Muon_eta, Muon_phi, Muon_mass)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df.Histo1D((\"hist\", \";m_{#mu#mu} (GeV);N_{Events}\", 5000, 2, 200), \"Dimuon_mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = df.Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT.gStyle.SetOptStat(0); ROOT.gStyle.SetTextFont(42)\n",
    "c = ROOT.TCanvas(\"c\", \"\", 800, 700)\n",
    "c.SetLogx(); c.SetLogy()\n",
    "hist.Draw()\n",
    "\n",
    "label = ROOT.TLatex(); label.SetNDC(True)\n",
    "label.SetTextSize(0.040); label.DrawLatex(0.100, 0.920, \"#bf{CMS Open Data}\")\n",
    "label.SetTextSize(0.030); label.DrawLatex(0.630, 0.920, \"#sqrt{s} = 8 TeV, L_{int} = 11.6 fb^{-1}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.Draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDataFrame can be easily turned into numpy ndarrays or pandas dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'root://eospublic.cern.ch//eos/opendata/cms/derived-data/AOD2NanoAODOutreachTool/Run2012BC_DoubleMuParked_Muons.root'\n",
    "path = './data/opendata_muons_skimmed.root'\n",
    "df = ROOT.RDataFrame('Events', path)\n",
    "\n",
    "npy = df.Filter('nMuon == 2')\\\n",
    "        .Filter('Muon_pt[0] != Muon_pt[1]')\\\n",
    "        .Define('Dimuon_mass', 'InvariantMass(Muon_pt, Muon_eta, Muon_phi, Muon_mass)')\\\n",
    "        .Range(10000)\\\n",
    "        .AsNumpy(['Dimuon_mass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.DataFrame(npy)\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(npy['Dimuon_mass'], range=(70, 110), bins=20)\n",
    "plt.xlabel('Dimoun mass in GeV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **awkward array**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In analysis we often have to work with data that can't be represented as a simple table (as if some branches of the tree are arrays of various size).\n",
    "\n",
    "__[Awkward Arrays](https://github.com/scikit-hep/awkward-1.0)__ provides support for such general tree-like data structures, that are also contiguous in memory and operated upon with compiled, vectorized code like NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward1 as ak # when we were preparing this notebook there was the transition awkward (v0) -> awkward (v1)  happening, hence this weird naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = ak.Array([\n",
    "    [{\"mu_m\": 90, \"jets_m\": [1]}, \n",
    "     {\"mu_m\": 50, \"jets_m\": [1, 2]}, \n",
    "     {\"mu_m\": [30,50,70], \"jets_m\": [1, 2, 3]}],\n",
    "    \n",
    "    [],\n",
    "    \n",
    "    [{\"mu_m\": 40, \"jets_m\": [1, 2, 3, 4]}, \n",
    "     {\"mu_m\": [], \"jets_m\": [1, 2, 3, 4, 5]}]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array[\"mu_m\"])\n",
    "print(array[\"jets_m\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.square(array[\"jets_m\", ..., 1:])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **uproot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[uproot](https://github.com/scikit-hep/uproot4) (originally μproot, for \"micro-Python ROOT\") is a reader and a writer of the ROOT file format using only Python and Numpy. Unlike the standard C++ ROOT implementation, uproot is only an I/O library, primarily intended to stream data into machine learning libraries in Python. Unlike PyROOT and root_numpy, uproot does not depend on C++ ROOT. Instead, it uses Numpy to cast blocks of data from the ROOT file as Numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uproot is a Python package; it is pip and conda-installable, and it only depends on other Python packages. Although it is similar in function to root_numpy and root_pandas, it does not compile into ROOT and therefore avoids issues in which the version used in compilation differs from the version encountered at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot4 as uproot # when we were preparing this notebook there was the transition uproot (v3) -> uproot (v4)  happening, hence this weird naming\n",
    "\n",
    "file = uproot.open(\"http://scikit-hep.org/uproot3/examples/nesteddirs.root\")\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uproot.open returns a ROOTDirectory, which behaves like a Python dict; it has keys(), values(), and key-value access with square brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['one']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file['one'].values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTrees are special objects in ROOT files: they contain most of the physics data. Uproot presents TTrees as subclasses of TTreeMethods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/opendata_muons_skimmed.root'\n",
    "\n",
    "file = uproot.open(path)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.classnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = file[\"Events\"]\n",
    "tree.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.typenames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.keys(filter_name=\"Muon_*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.keys(filter_typename=\"float[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.keys(filter_branch=lambda branch: not isinstance(branch.interpretation, uproot.AsJagged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"Muon_pt\"].array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"Muon_pt\"].array()[:20].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"Muon_pt\"].array(library=\"np\")[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_array = tree[\"Muon_pt\"].array(library=\"ak\")\n",
    "numpy_array = tree[\"Muon_pt\"].array(library=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awkward_array[:20, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree[\"Muon_pt\"].array(library=\"pd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(uproot.open(\"./data/opendata_muons_skimmed.root:Events/nMuon\"), bins=11, range=(-0.5, 10.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy arrays in a dict\n",
    "pv_numpy = tree.arrays(filter_name=\"Muon_*\", library=\"np\")\n",
    "pv_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Awkward record-array\n",
    "pv_awkward = tree.arrays(filter_name=\"Muon_*\", library=\"ak\")\n",
    "pv_awkward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame (as opposed to Series for a single array)\n",
    "pv_pandas = tree.arrays(filter_name=\"Muon_*\", library=\"pd\")\n",
    "pv_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uproot also has a limited ability to write ROOT files, including TTrees of flat data (non-jagged: single number per event), a variety of histogram types, and TObjString (for metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **iminuit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[iminuit](https://iminuit.readthedocs.io/en/stable/) is a Jupyter-friendly Python frontend to the MINUIT2 C++ library (which is also used for fitting in ROOT).\n",
    "\n",
    "It can be used as a general robust function minimisation method, but is most commonly used for likelihood fits of models to data, and to get model parameter error estimates from likelihood profile analysis.\n",
    "\n",
    "In the next example it is show how it can be used to perform simple least squares fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything in iminuit is done through the Minuit object, so we import it\n",
    "from iminuit import Minuit\n",
    "from iminuit.util import describe, make_func_code\n",
    "# we also need a cost function to fit and import the LeastSquares function\n",
    "from iminuit.cost import LeastSquares\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our line model\n",
    "def line(x, a, b):\n",
    "    return a + x * b\n",
    "\n",
    "# generate random toy data with random offsets in y\n",
    "np.random.seed(1)\n",
    "data_x = np.linspace(0, 1, 10)\n",
    "data_yerr = 0.1 # could also be an array\n",
    "data_y = line(data_x, 1, 2) + data_yerr * np.random.randn(len(data_x))\n",
    "\n",
    "# draw toy data\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\")\n",
    "plt.xlim(-0.1, 1.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple least-squares cost function looks like this...\n",
    "def custom_least_squares(a, b):\n",
    "    ym = line(data_x, a, b)\n",
    "    z = (data_y - ym) / data_yerr ** 2\n",
    "    return np.sum(z ** 2)\n",
    "\n",
    "# ...but instead of writing this by hand, it is better and\n",
    "# more convenient to use the LeastSquares class shipped with iminuit\n",
    "least_squares = LeastSquares(data_x, data_y, data_yerr, line)\n",
    "\n",
    "m = Minuit(least_squares, a=0, b=0)  # we need starting values for a and b\n",
    "\n",
    "m.migrad() # finds minimum of least_squares function\n",
    "m.hesse()  # computes errors \n",
    "\n",
    "# draw data and fitted line\n",
    "plt.errorbar(data_x, data_y, data_yerr, fmt=\"o\")\n",
    "plt.plot(data_x, line(data_x, *m.values.values()))\n",
    "\n",
    "# print parameter values and uncertainty estimates\n",
    "for p in m.parameters:\n",
    "    print(\"{} = {:.3f} +/- {:.3f}\".format(p, m.values[p], m.errors[p]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./data/pol_data_09_Dec_2020_14:10:54.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pol_data(data, mod=None):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 4))\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.grid()\n",
    "    ax2 = fig.add_subplot(122, polar=True)\n",
    "\n",
    "    ax1.plot(data[:,0],data[:,1],'ro')\n",
    "   \n",
    "    ax2.plot(data[:,0],data[:,1])\n",
    "    if mod is not None:\n",
    "        x_model = np.linspace(0,2*math.pi,num=200)\n",
    "        y_model = fit_func(x_model,mod['a'], mod['omega'], mod['phi'],mod['offset'])\n",
    "        ax1.plot(x_model,y_model,'b')\n",
    "    plt.show() \n",
    "plot_pol_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyChi2:\n",
    "    def __init__(self, model, x, y):\n",
    "        self.model = model  # model predicts y for given x\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "        self.y_err = np.where(self.y > 0, np.sqrt(self.y),1)\n",
    "        self.func_code = make_func_code(describe(self.model)[1:])\n",
    "\n",
    "    def __call__(self, *par):  # we accept a variable number of model parameters\n",
    "        ym = self.model(self.x, *par)\n",
    "        self.y = np.where(self.y_err > 0, self.y,0)\n",
    "        chi2 = np.sum(np.where(self.y > 0, (self.y-ym)**2/self.y_err**2,0))\n",
    "        return chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_func(x, a, omega, phi, offset):\n",
    "    return offset + a*np.power(np.cos(omega*x + phi),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2 = MyChi2(fit_func, data[:,0], data[:,1])\n",
    "m = Minuit(chi2, \n",
    "    limit_a=(100,4500),\n",
    "    error_a=1.,\n",
    "    limit_omega=(0,2),\n",
    "    error_omega=0.001,\n",
    "    limit_phi=(-1.6,1.6),\n",
    "    error_phi=0.001,\n",
    "    limit_offset=(0,3000),\n",
    "    error_offset=1,\n",
    "    pedantic=True,\n",
    "    errordef=1)\n",
    "m.migrad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pol_data(data, m.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **zfit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea behind [zfit](https://github.com/zfit/zfit) is to offer a Python oriented alternative to the very successful RooFit library from the ROOT data analysis package that can integrate with the other packages that are part if the scientific Python ecosystem. Contrary to the monolithic approach of ROOT/RooFit, the aim of zfit is to be light and flexible enough to integrate with any state-of-art tools and to allow scalability going to larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import zfit\n",
    "# Wrapper for some tensorflow functionality\n",
    "from zfit import z\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = zfit.Space('x', limits=(-10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_true = 0\n",
    "sigma_true = 1\n",
    "\n",
    "data_np = np.random.normal(mu_true, sigma_true, size=10000)\n",
    "data = zfit.data.Data.from_numpy(obs=obs, array=data_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = zfit.Parameter(\"mu\", 2.4, -1., 5., step_size=0.001)  # step_size is not mandatory but can be helpful\n",
    "sigma = zfit.Parameter(\"sigma\", 1.3, 0, 5., step_size=0.001)  # it should be around the estimated uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = zfit.pdf.Gauss(obs=obs, mu=mu, sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the negative log likelihood\n",
    "\n",
    "nll = zfit.loss.UnbinnedNLL(model=gauss, data=data)  # loss\n",
    "\n",
    "# Load and instantiate a minimizer\n",
    "minimizer = zfit.minimize.Minuit()\n",
    "minimum = minimizer.minimize(loss=nll)\n",
    "\n",
    "# Get the fitted values, again by run the variable graphs\n",
    "params = minimum.params\n",
    "\n",
    "print(\"mu={}\".format(params[mu]['value']))\n",
    "print(\"sigma={}\".format(params[sigma]['value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 50\n",
    "range_ = (-5,5)\n",
    "_ = plt.hist(data_np, bins=n_bins, range=range_)\n",
    "x = np.linspace(*range_, num=1000)\n",
    "with gauss.set_norm_range(range_):\n",
    "    pdf = zfit.run(gauss.pdf(x))\n",
    "_ = plt.plot(x, data_np.shape[0] / n_bins * (range_[1] - range_[0]) * pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **pyhf**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HistFactory p.d.f. template [__[CERN-OPEN-2012-016](https://cds.cern.ch/record/1456844)__] is per-se independent of its implementation in ROOT and sometimes, it’s useful to be able to run statistical analysis outside of ROOT, RooFit, RooStats framework.\n",
    "\n",
    "[pyhf](https://github.com/scikit-hep/pyhf) is a pure-python implementation of that statistical model for multi-bin histogram-based analysis and its interval estimation is based on the asymptotic formulas of “Asymptotic formulae for likelihood-based tests of new physics” [__[arXiv:1007.1727](https://arxiv.org/abs/1007.1727)__]. The aim is also to support modern computational graph libraries such as PyTorch and TensorFlow in order to make use of features such as autodifferentiation and GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf\n",
    "import pyhf.contrib.viz.brazil\n",
    "\n",
    "pyhf.set_backend(\"numpy\")\n",
    "model = pyhf.simplemodels.hepdata_like(\n",
    "    signal_data=[10.0], bkg_data=[50.0], bkg_uncerts=[7.0]\n",
    ")\n",
    "data = [55.0] + model.config.auxdata\n",
    "\n",
    "poi_vals = np.linspace(0, 5, 41)\n",
    "results = [\n",
    "    pyhf.infer.hypotest(test_poi, data, model, qtilde=True, return_expected_set=True)\n",
    "    for test_poi in poi_vals\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(7, 5)\n",
    "ax.set_xlabel(r\"$\\mu$ (POI)\")\n",
    "ax.set_ylabel(r\"$\\mathrm{CL}_{s}$\")\n",
    "pyhf.contrib.viz.brazil.plot_results(ax, poi_vals, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **hepstats**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [hepstats](https://github.com/scikit-hep/hepstats) module includes modeling, hypotests and [sPlot](https://root.cern.ch/doc/v612/classRooStats_1_1SPlot.html) submodules.\n",
    "\n",
    "The modeling submodule includes the Bayesian Block algorithm that can be used to improve the binning of histograms. The visual improvement can be dramatic, and more importantly, this algorithm produces histograms that accurately represent the underlying distribution while being robust to statistical fluctuations. Here is a small example of the algorithm applied on Laplacian sampled data, compared to a histogram of this sample with a fine binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepstats.modeling import bayesian_blocks\n",
    "\n",
    "data = np.random.laplace(size=10000)\n",
    "blocks = bayesian_blocks(data)\n",
    "\n",
    "plt.hist(data, bins=1000, label='Fine Binning', density=True, alpha=0.6)\n",
    "plt.hist(data, bins=blocks, label='Bayesian Blocks', histtype='step', density=True, linewidth=2)\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **hepunits**\n",
    "\n",
    "[hepunits](https://github.com/scikit-hep/hepunits) collects the most commonly used units and constants in the HEP System of Units, as derived from the basic units originally defined by the CLHEP project. The package is in agreement with the values in the 2020 Particle Data Group review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepunits.constants import c_light\n",
    "from hepunits.units     import picosecond, micrometer\n",
    "tau_Bs = 1.5 * picosecond    # a particle lifetime, say the Bs meson's\n",
    "ctau_Bs = c_light * tau_Bs   # ctau of the particle, ~450 microns\n",
    "print(ctau_Bs)                # result in HEP units, so mm\n",
    "print(ctau_Bs / micrometer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two quantities with length units and get the result in meters\n",
    "from hepunits import units as u\n",
    "print((1 * u.meter + 5 * u.cm) / u.meter)\n",
    "# the default result is, of course, in HEP units, so mm\n",
    "print(1 * u.meter + 5 * u.cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lpi",
   "language": "python",
   "name": "ml-lpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
