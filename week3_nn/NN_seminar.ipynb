{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Credit:_ this notebook is based on **CMS Data Analysis School** [ML exercise](https://twiki.cern.ch/twiki/bin/viewauth/CMS/SWGuideCMSDataAnalysisSchoolCERN2020MLShortExercise) (CMS restricted) and was developed by [Marcel Rieger](mailto:marcel.rieger@cern.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "Our case study will be that of discriminating between jets produced by a hadronically decaying top quark which hadronizes, to jets produced by a light flavour quark or a gluon.\n",
    "\n",
    "If the top quark has a very high transverse momentum, the decay products of the top (one b jet and two quark jets stemming from the decaying W boson), will be merged into one single large jet, which is referred to as a **top jet**. Potentially, this jet can exhibit three distinct, resolvable *sub jets*, whereas a light quark or gluon jet only appears as one single, large jet without any significant substructure.\n",
    "\n",
    "The different appearance of these jets can be used as a handle to discriminate between them.  Being able to correctly identify top jets, and tell them apart from the overwhelming background of other light-flavored jets, is extremely important for many reasons.\n",
    "\n",
    "Since the top quark is so heavy, being the only fermion we know of with a mass on the order of the weak scale, several extensions of the Standard Model which attempt to solve the hierarchy problem predict large couplings of new, hitherto unobserved particles to top quarks. Weeding top quark jets out of the ocean of other jets is therefore crucial for many **New Physics** searches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "<center><img src=\"assets/top_vs_qcd.png\" width=\"60%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "The input data consists of jets, originating from either\n",
    "  - hadronically decaying top quarks (this is our **signal** ‚úîÔ∏é), or\n",
    "  - dijet QCD events (our **background** ‚úò),\n",
    " \n",
    "and clustered using the [anti-$k_{T}$ algorithm](https://arxiv.org/abs/0802.1189) with $\\Delta R$ = 0.8.\n",
    "\n",
    "<br />\n",
    "\n",
    "Data was generated using Pythia & Delphes, configured\n",
    "  - To collide protons at 14 TeV center-of-mass energy,\n",
    "  - To generate jets with a $p_{T}$ range of [550, 650] GeV (before hadronization ‚ùóÔ∏è), and\n",
    "  - **Without** mixing in pileup events for simplicity\n",
    "  - All made for default ATLAS detector card"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input features\n",
    "\n",
    "Per jet, you are given the four-vectors of up to **200** of its *constituents* (i.e., the particles that form the jet by means of clustering).\n",
    "\n",
    "   - These up to 800 values define your **input features**.\n",
    "   - Note that not all jets have that many constituents‚ùóÔ∏è\n",
    "   - To spare you the trouble of working with uneven (so-called *jagged*) arrays, these \"missing\" constituents vectors are filled with zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split"
   },
   "source": [
    "### Training target\n",
    "\n",
    "Per jet, you are provided a flag that marks the true origin of the jet   \n",
    "    - `1` for jets from top quark decays  \n",
    "    - `0` for light jets from QCD events  \n",
    " \n",
    "We want to separate **top-like events** from **QCD background** i.e. to solve a _classification_ problem for jets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Diving into the data\n",
    "\n",
    "Let's check out the data! It is stored in NumPy arrays across several files, with 50k jets per file. This way, prototyping and test runs are way quicker. You are given\n",
    "\n",
    "- 20 training files (`\"train\"`)\n",
    "- 8 validation files (`\"valid\"`)\n",
    "- 8 testing files (`\"test\"`)  \n",
    "\n",
    "For a lightweight version we're going to use two files: one for training, the other for validation. You can find the full dataset using the following [link](https://drive.google.com/drive/folders/1LzfFSglalmV9jICKj0uI4SWdMOE00-1d?usp=sharing).\n",
    "\n",
    "A few tools to perform recurrent tasks such as data loading are available in the dedicated `dasml` package. Let's import all packages we need during this exercise and load two training files and inspect the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# check the tf version\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load the dasml package and other software\n",
    "import dasml\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from tqdm.auto import tqdm\n",
    "import ipywidgets\n",
    "import livelossplot as llp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the content of two \"train\" files\n",
    "c_vectors, true_vectors, labels = dasml.data.load(\"train\", start_file=0, stop_file=1)\n",
    "c_vectors.shape, true_vectors.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c_vectors[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All arrays have 100k (2 x 50k) *rows* (dimension 0).\n",
    "\n",
    "- Per jet, we have up to 200 constituents (`c_vectors`) with 4 variables ($E$, $p_x$, $p_y$, $p_z$) each, thus `(200, 4)`.\n",
    "- Consistently, `true_vectors` only has 4 values per jet.\n",
    "- The `labels`, however, are single values.\n",
    "\n",
    "Let's create a few plots to get some insights into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some flags to make four-vector element access more verbose\n",
    "E, PX, PY, PZ = range(4)\n",
    "print('E={}, PX={}, PY={}, PZ={} '.format(E,PX,PY,PZ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a histogram helper\n",
    "def plot_hist(arr, names=None, xlabel=None, xlim=None, ylabel=\"Entries\", filename=None, legend_loc=\"upper center\", **kwargs):\n",
    "    kwargs.setdefault(\"bins\", 20)\n",
    "    kwargs.setdefault(\"alpha\", 0.7)\n",
    "   \n",
    "    # consider multiple arrays and names given as a tuple\n",
    "    arrs = arr if isinstance(arr, tuple) else (arr,)\n",
    "    names = names or (len(arrs) * [\"\"])\n",
    "\n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    for arr, name in zip(arrs, names):\n",
    "        bin_edges = ax.hist(arr, label=name, **kwargs)[1]\n",
    "        kwargs[\"bins\"] = bin_edges\n",
    "    if xlim:\n",
    "         ax.set_xlim(xlim)\n",
    "    # legend\n",
    "    if any(names):\n",
    "        legend = ax.legend(loc=legend_loc)\n",
    "        legend.get_frame().set_linewidth(0.0)\n",
    "    \n",
    "    # styles and custom adjustments\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    " \n",
    "    if filename:\n",
    "        fig.savefig(filename)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Truth distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# distribution of truth labels\n",
    "fig = plot_hist(labels, xlabel=\"Label distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# energy distribution of the true top quark particle\n",
    "# remember, this is only available for top jets (zero otherwise)\n",
    "is_top = labels == 1\n",
    "fig = plot_hist(true_vectors[is_top, E], xlabel=\"True energy / GeV\")\n",
    "fig = plot_hist(true_vectors[np.logical_not(is_top), E], xlabel=\"True energy / GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# px distribution of the true particle\n",
    "fig = plot_hist(true_vectors[is_top, PX], xlabel=\"True $p_x$ / GeV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# mass distribution of the true particle\n",
    "hist_kwargs = {'bins':300}\n",
    "mass = (true_vectors[:, E]**2 - np.sum(true_vectors[:, PX:]**2, axis=1))**0.5 \n",
    "fig = plot_hist(mass[is_top], xlabel=\"True mass / GeV\",xlim=[160,180],**hist_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, this is a top!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Input feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# number of constituents per jet\n",
    "# remember, missing constituents are filled with zeros, so we take the energy value as a marker\n",
    "n_c = np.count_nonzero(c_vectors[:, :, E], axis=1)\n",
    "fig = plot_hist(n_c, xlabel=\"N constituents per jet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# energy distribution of all constituents\n",
    "e_c = c_vectors[:, :, E].flatten()\n",
    "# store a mask to remove zeros\n",
    "non_zero = (e_c != 0)\n",
    "fig = plot_hist(e_c[non_zero], log=True, xlabel=\"Constituents energy / GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# px distribution of all constituents, zeros removed with the mask defined above\n",
    "px_c = c_vectors[:, :, PX].flatten()\n",
    "fig = plot_hist(px_c[non_zero], log=True, xlabel=\"Constituents $p_x$ / GeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# pz distribution of all constituents\n",
    "pz_c = c_vectors[:, :, PZ].flatten()\n",
    "fig = plot_hist(pz_c[non_zero], log=True, xlabel=\"Constituents $p_z$ / GeV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "Altough you were promised *up to* 200 constituents per jet, only a few of them seem to have more than 100 constituents!\n",
    "\n",
    "Expect these *findings*, but don't interpret anything as bad intention üòâ The work packages of large-scale analysis are often shared and spread among multiple people, working groups and institutes. Staying on top of things is naturally a complex part, so communication and documentation is - as always - key!\n",
    "\n",
    "Ok, so now that we understood the data, it would not make sense to include all these zeros in a network training. We can safely pick only the first, say, **120 constituents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Keras overview: a minimal workflow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keras** is a deep learning interface written in Python, running on top of the machine learning platform **TensorFlow**. It was developed with a focus on enabling fast experimentation.\n",
    "\n",
    "Keras operates with **layers** and **models**. To make the simpliest model, we need to specify input layer and internal layers. \n",
    "\n",
    "\n",
    "\n",
    "Before creating a full-blown training setup, let's first do a quickshot. This helps us to understand how a model is built, trained, and eventually evaluated. We can also already define a few plot methods to assess the performance.\n",
    "\n",
    "For this purpose, we use TensorFlow with the Keras high level API in its [functional version](https://keras.io/guides/functional_api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template function for the input layer \n",
    "#Only for illustration\n",
    "tf.keras.Input(\n",
    "    shape=None,\n",
    "    batch_size=None,\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    sparse=False,\n",
    "    tensor=None,\n",
    "    ragged=False,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Template function for densely-connected NN layer\n",
    "#Only for illustration\n",
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense implements the operation: <span style=\"color:blue\">output = activation(dot(input, kernel) + bias) </span> where <span style=\"color:blue\">activation</span> is the element-wise activation function passed as the activation argument, <span style=\"color:blue\">kernel</span> is a weights matrix created by the layer, and <span style=\"color:blue\">bias</span> is a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model generating function\n",
    "# - 2 hidden layers\n",
    "# - 128 units each\n",
    "# - tanh activation\n",
    "# - 2 output units with softmax activation\n",
    "# (applies exp() to outputs and normalizes sum of all outputs to 1)\n",
    "def create_model():\n",
    "    x = tf.keras.Input(shape=(480,))\n",
    "    #Then the first internal layer takes input layer as an input\n",
    "    a1 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(x)\n",
    "    a2 = tf.keras.layers.Dense(128, use_bias=True, activation=\"tanh\")(a1)\n",
    "    y = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a2)\n",
    "    return tf.keras.Model(inputs=x, outputs=y, name=\"toptagging_quickshot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the actual model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see what happens when we call it with zeros\n",
    "# note: here we create zeros with in the shape (1, 480)\n",
    "# where the leading one marks the *batch size*,\n",
    "# i.e. the number of examples that are simultaneously fed\n",
    "# into the network to benefit from clever vectorization\n",
    "t = model.predict(np.zeros((1, 480)))\n",
    "print(type(t))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a Numpy array of predictions for each object. Computation is done in batches. This method is designed for performance in large scale inputs. \n",
    "\n",
    "The return value is `[0.5, 0.5]`. This means that, given a vector of input features consisting only of zeros, the network is unsure whether to assign it to the signal class (top jets) nor to the background class (light jets). This is totally reasonable as we haven't trained it yet. So let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we define a preprocessing function that (e.g.) takes the\n",
    "# constiuents and returns an other representation of them\n",
    "# in this case, we select only the first 120 constituents and\n",
    "# flatten the resulting array from (..., 120, 4) to (..., 480,)\n",
    "def preprocess_constituents(constituents):\n",
    "    return constituents[:, :120].reshape((-1, 480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also, for the training we need to convert the label to a \"one-hot\" representation\n",
    "# 0. -> [1., 0.]\n",
    "# 1. -> [0., 1.]\n",
    "def labels_to_onehot(labels):\n",
    "    labels = labels.astype(np.int32)\n",
    "    onehot = np.zeros((labels.shape[0], labels.max() + 1), dtype=np.float32)\n",
    "    onehot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load more training, and also validation data\n",
    "c_vectors_train, _, labels_train = dasml.data.load(\"train\", stop_file=1)\n",
    "c_vectors_valid, _, labels_valid = dasml.data.load(\"valid\", stop_file=1)\n",
    "\n",
    "# run the preprocessing\n",
    "c_vectors_train = preprocess_constituents(c_vectors_train)\n",
    "c_vectors_valid = preprocess_constituents(c_vectors_valid)\n",
    "\n",
    "# create one-hot labels\n",
    "labels_train = labels_to_onehot(labels_train)\n",
    "labels_valid = labels_to_onehot(labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "# this means that the internal computational graph structure is built,\n",
    "# the loss function (the function that provides the feedback by comparing\n",
    "# expected and predicted result, more on that later), and metrics are\n",
    "# registered that are shown during the training\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the training for 5 epochs (running through all data 5 times)\n",
    "model.fit(\n",
    "    c_vectors_train,\n",
    "    labels_train,\n",
    "    batch_size=200,\n",
    "    epochs=5,\n",
    "    callbacks=[llp.PlotLossesKerasTF(outputs=[llp.outputs.MatplotlibPlot(cell_size=(4, 2))])],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with an accuracy of about 70%, which is already quite good for such a small network (and lot's of important things we did not even consider yet ...)!\n",
    "\n",
    "Let's check if the model generalized by evaluating the validation data and manually computing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate all training and validation data again for ruther study\n",
    "predictions_train = model.predict(c_vectors_train)\n",
    "predictions_valid = model.predict(c_vectors_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the accuracy\n",
    "def calculate_accuracy(labels, predictions):\n",
    "    # while the labels (NumPy array) are one-hot encoded,\n",
    "    # each prediction (TF tensor) consists of two numbers whose sum is 1,\n",
    "    # so we interpret the prediction to be the signal when the second value (index 1) is > 0.5\n",
    "    # hence, we can use argmax\n",
    "    predicteds_top = np.argmax(predictions, axis=-1) == 1\n",
    "    labels_top = labels[:, 1] == 1\n",
    "    return (predicteds_top == labels_top).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = calculate_accuracy(labels_train, predictions_train)\n",
    "acc_valid = calculate_accuracy(labels_valid, predictions_valid)\n",
    "\n",
    "print(f\"train accuracy: {acc_train:.4f}\")\n",
    "print(f\"valid accuracy: {acc_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks fairly similar, so for now, we don't seem to experience overtraining.\n",
    "\n",
    "We proceed by taking a look at the output distributions of the validation dataset, separated into signal and background components. Since we are dealing with a binary classification, and the sum of the two output values is normalized to one, it is sufficient to inspect just one of the output nodes. Since our goal is to identify signal, we look at the second column with index 1 (note that the same is considered in the accuracy calculation above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_hist(\n",
    "    (predictions_valid[labels_valid == 0], predictions_valid[labels_valid == 1]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the classification accuracy, we can study the *receiver operating characteristic* curve or **ROC** curve. It shows the relation between the true positive (jets *correctly* identified as top jets) and false positive rates (light jets *mistaken* as a top jets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to draw a ROC curve\"\n",
    "def plot_roc(labels, predictions, names=None, xlim=(0.01, 1), ylim=(0.01, 1)):   \n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel(\"True positive rate\")\n",
    "    ax.set_ylabel(\"False positive rate\")\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    ax.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_xlim(left=xlim[0], right=xlim[1])\n",
    "    ax.set_ylim(bottom=ylim[0], top=ylim[1])\n",
    "    ax.set_aspect(1)\n",
    "    plots = []\n",
    "\n",
    "    # treat labels and predictions as tuples\n",
    "    labels = labels if isinstance(labels, tuple) else (labels,)\n",
    "    predictions = predictions if isinstance(predictions, tuple) else (predictions,)\n",
    "    names = names or (len(labels) * [\"\"])\n",
    "    for l, p, n in zip(labels, predictions, names):\n",
    "        # linearize\n",
    "        l = l[:, 1]\n",
    "        p = p[:, 1]\n",
    "\n",
    "        # create the ROC curve and get the AUC\n",
    "        fpr, tpr, _ = roc_curve(l, p)\n",
    "        auc = roc_auc_score(l, p)\n",
    "        \n",
    "        # apply lower x limit to prevent zero division warnings below\n",
    "        fpr = fpr[tpr > xlim[0]]\n",
    "        tpr = tpr[tpr > xlim[0]]\n",
    "\n",
    "        # plot\n",
    "        plot_name = (n and (n + \", \")) + \"AUC {:.3f}\".format(auc)\n",
    "        plots.extend(ax.plot(fpr, tpr, label=plot_name))\n",
    "\n",
    "    # legend\n",
    "    legend = ax.legend(plots, [p.get_label() for p in plots], loc=\"upper right\")\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the roc plot\n",
    "fig = plot_roc(\n",
    "    (labels_train, labels_valid),\n",
    "    (predictions_train, predictions_valid),\n",
    "    names=(\"train\", \"valid\"),\n",
    ")\n",
    "fig.set_size_inches(6,6) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to draw a ROC curve\"\n",
    "def plot_log_roc(labels, predictions, names=None, xlim=(0.01, 1), ylim=(1, 1e2)):   \n",
    "    # start plot\n",
    "    fig, ax = plt.subplots()\n",
    "    #ax.set_xlabel(\"Signal efficiency\")\n",
    "    ax.set(xlabel='Signal efficiency '+ r'$[\\varepsilon_{S}]$', ylabel='Background rejection ' + r'$[1/\\varepsilon_{B}]$')\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.tick_params(axis=\"both\", direction=\"in\", top=True, right=True)\n",
    "    ax.set_xticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_xlim(left=xlim[0], right=xlim[1])\n",
    "    ax.set_ylim(bottom=ylim[0], top=ylim[1])\n",
    "    plots = []\n",
    "\n",
    "    # treat labels and predictions as tuples\n",
    "    labels = labels if isinstance(labels, tuple) else (labels,)\n",
    "    predictions = predictions if isinstance(predictions, tuple) else (predictions,)\n",
    "    names = names or (len(labels) * [\"\"])\n",
    "    for l, p, n in zip(labels, predictions, names):\n",
    "        # linearize\n",
    "        l = l[:, 1]\n",
    "        p = p[:, 1]\n",
    "\n",
    "        # create the ROC curve and get the AUC\n",
    "        fpr, tpr, _ = roc_curve(l, p)\n",
    "        auc = roc_auc_score(l, p)\n",
    "        \n",
    "        # apply lower x limit to prevent zero division warnings below\n",
    "        fpr = fpr[tpr > xlim[0]]\n",
    "        tpr = tpr[tpr > xlim[0]]\n",
    "\n",
    "        # plot\n",
    "        plot_name = (n and (n + \", \")) + \"AUC {:.3f}\".format(auc)\n",
    "        plots.extend(ax.plot(tpr, 1. / fpr, label=plot_name))\n",
    "\n",
    "    # legend\n",
    "    legend = ax.legend(plots, [p.get_label() for p in plots], loc=\"upper right\")\n",
    "    legend.get_frame().set_linewidth(0.0)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot fancy roc curve\n",
    "fig = plot_log_roc(\n",
    "    (labels_train, labels_valid),\n",
    "    (predictions_train, predictions_valid),\n",
    "    names=(\"train\", \"valid\"),\n",
    ")\n",
    "fig.set_size_inches(6,6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves above are produced by scanning potential values to cut on the network output and examining the resulting signal classification (true positive) and background mis-classification (false positive) rates.\n",
    "\n",
    "Naturally, a well performing network has a high true positive rate while keeping the (reciprocal) false positive rate at a reasonably low (high) level. For the choice of the axes above, this would lead to a curve that is bent towards the upper right corner. But be aware that other representations of the ROC curve exist which might look somewhat different (e.g. \"1 - false positive rate\" on the y-axis). Their message is, however, identical.\n",
    "\n",
    "A commonly used proxy that compiles the values for all possible cuts into one metric is the area-under-curve - **AUC**. A value of 1 signalizes a perfectly working network that allows for a cut value leading to 100% signal efficiency and 0% background contamination. Opposed to that, a value of 0.5 means that the two output distributions of signal and background events are probably fully overlapping, lacking the opportunity to apply a cut that would favor signal examples. A value of 0 has the same logical meaning as 1, but the definition of what is signal and background is flipped. Therefore, the distance from 0.5 is what actually matters here.\n",
    "\n",
    "A value around 0.75 is already quite decent, but there's still potential. You can try to beat this value in the full training setup below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "- Now we know how to build a simple model using TensorFlow and Keras.\n",
    "- We learned how to one-hot encode labels.\n",
    "- We performed a quick training using the `fit()` method of Keras models.\n",
    "- To ensure model generalization, we evaluated validation data with our trainined model.\n",
    "- We calculated accuracies and visualized the output distributions.\n",
    "- We learned about ROC curves, AUC values and how to plot / compute them.\n",
    "\n",
    "With these tools at hand, we can jump into the next section and build a custom training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Training++**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more insights into the actual neural network training process, we will use Keras only to compose the model. For preprocessing, the definition of losses, and the training loop, we will use bare TensorFlow operations and tools.\n",
    "\n",
    "Also, we reconsider some of the choices we made above and incorporate a few techniques that improve the network training.\n",
    "\n",
    "Here are a few TensorFlow resources that might help you in the process:\n",
    "\n",
    "- [Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "- [Loading NumPy data](https://www.tensorflow.org/tutorials/load_data/numpy)\n",
    "- [Keras layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "- [Eager execution](https://www.tensorflow.org/guide/eager)\n",
    "- [Gradient tape and differentiation](https://www.tensorflow.org/guide/autodiff)\n",
    "- [Graphs and introduction to `tf.function`](https://www.tensorflow.org/guide/intro_to_graphs)\n",
    "- [Better performance with `tf.function`](https://www.tensorflow.org/guide/function)\n",
    "- [Training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
    "- [TensorFlow 2 tutorial held at CERN](https://indico.cern.ch/event/882992/contributions/3721506/attachments/1994721/3327402/TensorFlow_2_Workshop_CERN_2020.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eager execution and graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with NumPy, we can interactively work with TensorFlow tensors. Each operation is executed *eagerly* as soon as the interpreter reaches and evaluates that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "t = tf.range(0., 10.)\n",
    "print(t)\n",
    "t = t * 2\n",
    "print(t)\n",
    "t = t + 1\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, one might not be interested in intermediate results so the outcome of `t = t * 2` in line 3 is perhaps not required. Also, imagine the operation `(t * 2) + 1` is executed on a GPU (Graphical Processing Unit). The content of `t` - not that many bytes in this example, but tensors can easily reach a couple MBs - is transferred to the GPU, together with the instructions to multiply each value by 2 and then adding 1. The output of this computation is sent back to the CPU where (e.g.) the Python interpreter can print the numbers as done in line 6.\n",
    "\n",
    "There is obviously no need to send back the result of `t * 2`. However, this is exactly what would happen in the example above. While this is a nice and intuitive way to prototype a new model, we somehow need a way to tell TensorFlow to compute a set of instructions as a whole, and that we are only interested in the final result. This is where **graphs** enter the equation.\n",
    "\n",
    "A computational graph describes the symbolic instructions that should be performed on certain input tensors (orange) to produce the result of a complex computation. These instructions are represented by `tf.Operation` objects (green), while the data flowing between them is contained in `tf.Tensor`'s (purple). The graph of the computation above would look like this:\n",
    "\n",
    "[![](https://mermaid.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)](https://mermaid-js.github.io/mermaid-live-editor/#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)\n",
    "\n",
    "To declare a computational graph, we can write a function and decorate it with `tf.function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def my_func(t):\n",
    "    print(\"new graph created\", t.dtype, t.shape)\n",
    "    t = t * 2\n",
    "    print(t)\n",
    "    t = t + 1\n",
    "    print(t)\n",
    "    return t\n",
    "\n",
    "t = tf.range(0., 10.)\n",
    "my_func(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output is exactly the same, but the intermediate tensors no longer have values attached to them. The first time we called `my_func` in line 10, a concrete graph was created that expects an input tensor with type `float32` and shape `(10,)`. In fact, when we repeat this call with an input tensor of identical type and shape, `my_func` is not even called, but TensorFlow uses the previously created graph and executes it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_func(tf.range(10., 20.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No line `new graph created ...` is printed, implying that `my_func` is indeed not called!\n",
    "\n",
    "However, if we use a tensor with a different type or shape, a new graph is created and stored internally. This powerful feature is called **signature tracing** and you can read more about it [here](https://www.tensorflow.org/guide/function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_func(tf.range(10., 21.))\n",
    "print(\"---\")\n",
    "my_func(tf.range(10, 20, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these concepts at hand, we can go ahead and start building our data pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data pipeline and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we created a method `preprocess_constituents` to select the first 120 constituents per jet and to merge the last two dimensions so that we can feed a NN that expects an input feature vector. This was sufficient as the `model.fit` method knows how to apply batching of input jets and how to repeat the dataset to train for more than one epoch.\n",
    "\n",
    "Using plain TensorFlow, we use a `tf.data.Dataset` object for this purpose. Let's write a function that returns a dataset for our training.\n",
    "\n",
    "But now, we include an important aspect of deep learning, namely **feature scaling** (FS)! To introduce FS, we first need to understand the concept of **numerical domains** in the context of NN applications.\n",
    "\n",
    "Our input data - a selection of four-vectors with values given in GeV - clearly comes from the domain of physics. As we have seen in the plots above, their numerical values range from -500 to 500 for $p_x$ and $p_y$, and up to 2000 for $E$ and $p_z$ values. We can use the abstract term *application domain* to describe these ranges. However, the domain of numbers being passed back and forth through the network is entirely different and can even vary depending on the architecture you pick! A classical feed forward network, such as the one we created above, and a plentora of techniques that were developed throughout the last decade(s) prefer values to be in a range between, say, -1 and 1, and we can call it *network domain*. This is just an example and somewhat larger values are certainly fine as well. But still, you get the idea that numerical application and network domains are *entirely different*.\n",
    "\n",
    "Numerically, the output of our *classification* model is still in the network domain and we simply interpret it as a binary classification decision, so we are safe on this end. Things get a bit more tricky when we perform a *regression* task that should predict the value of a physics quantitiyüëæ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(kind, shuffle=False, repeat=1, batch_size=100, n_constituents=120, seed=None, **kwargs):\n",
    "    # first, we load the data as before, passing all unresolved keyword arguments\n",
    "    c_vectors, true_vectors, labels = dasml.data.load(kind, **kwargs)\n",
    "    \n",
    "    # first, we measure the mean and standard deviation of the raw input vectors,\n",
    "    # of course not taking into account missing constituents\n",
    "    # we will need them for the feature scaling later on\n",
    "    non_zero = c_vectors[:, :, E].flatten() > 0\n",
    "    means = tf.constant([\n",
    "        np.mean(c_vectors[:, :, v].flatten()[non_zero])\n",
    "        for v in (E, PX, PY, PZ)\n",
    "    ])\n",
    "    variances = tf.constant([\n",
    "        np.var(c_vectors[:, :, v].flatten()[non_zero])\n",
    "        for v in (E, PX, PY, PZ)\n",
    "    ])\n",
    "    stddevs = tf.maximum(variances, 1e-6)**0.5\n",
    "    \n",
    "    # then we apply the cut on the first n_constituents per jet\n",
    "    # this is prettly basic and can happen outside the data pipeline\n",
    "    c_vectors = c_vectors[:, :n_constituents]\n",
    "    \n",
    "    # one-hot encode labels\n",
    "    labels = labels_to_onehot(labels)\n",
    "    \n",
    "    # create a tf dataset\n",
    "    data = (c_vectors, true_vectors, labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "    \n",
    "    # in the following, we amend the dataset object using methods\n",
    "    # that return a new dataset object *without* copying the data\n",
    "    \n",
    "    # apply shuffeling\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10 * batch_size, reshuffle_each_iteration=True, seed=seed)\n",
    "    \n",
    "    # apply repitition, i.e. start iterating from the beginning when the dataset is exhausted\n",
    "    ds = ds.repeat(repeat)\n",
    "    \n",
    "    # apply batching\n",
    "    if batch_size < 1:\n",
    "        batch_size = c_vectors.shape[0]\n",
    "    ds = ds.batch(batch_size)\n",
    "    \n",
    "    # store the original data for later access\n",
    "    ds._orig_data = data\n",
    "    \n",
    "    return ds, means, stddevs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training dataset\n",
    "dataset_train, means_train, stddevs_train = create_dataset(\n",
    "    \"train\", shuffle=True, repeat=-1, batch_size=200, stop_file=1)\n",
    "\n",
    "# also load all validation data but disable batching for easier handling\n",
    "dataset_valid, _, _ = create_dataset(\"valid\", batch_size=-1, stop_file=1)\n",
    "\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature scaling in a custom Keras layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the feature scaling procedure as a custom keras layer\n",
    "# that has, of course, no weights as it is not trainable\n",
    "# see https://keras.io/guides/making_new_layers_and_models_via_subclassing for more info\n",
    "\n",
    "class FeatureScaling(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, means, stddevs):\n",
    "        \"\"\"\n",
    "        Constructor. Stores arguments as instance members.\n",
    "        \"\"\"\n",
    "        super(FeatureScaling, self).__init__(trainable=False)\n",
    "\n",
    "        self.means = means\n",
    "        self.stddevs = stddevs\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Method that is required for model cloning and saving. It should return a\n",
    "        mapping of instance member names to the actual members.\n",
    "        \"\"\"\n",
    "        return {\"means\": self.means, \"stddevs\": self.stddevs}\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "        Method that, given an input shape, defines the shape of the output tensor.\n",
    "        This way, the entire model can be built without actually calling it.\n",
    "        \"\"\"\n",
    "        return (input_shape[0], input_shape[1] * input_shape[2])\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Any variables defined by this layer should be created inside this method.\n",
    "        This helps Keras to defer variable registration to the point where it is\n",
    "        needed the first time, and in particular not at definition time.\n",
    "        \"\"\"\n",
    "        # nothing to do here as our feature scaling has not trainable parameters\n",
    "\n",
    "    def call(self, c_vectors):\n",
    "        \"\"\"\n",
    "        Payload of the layer that takes inputs and computes the requested output\n",
    "        whose shape should match what is defined in compute_output_shape.\n",
    "        \"\"\"\n",
    "        # scale each feature such that it is distributed around 0 with a standard deviation of 1\n",
    "        # BUT: there are already many zeros in the input features and they have\n",
    "        #      a distinct meaning (missing constituents); we want to keep this information, so we\n",
    "        #      shift these values to -3, i.e. 3 standard deviations to the left\n",
    "        e, px, py, pz = tf.unstack(c_vectors, axis=-1)\n",
    "        zero_pos = -3. * tf.ones_like(e)\n",
    "        non_zero = e > 0\n",
    "        e = tf.where(non_zero, (e - self.means[E]) / self.stddevs[E], zero_pos)\n",
    "        px = tf.where(non_zero, (px - self.means[PX]) / self.stddevs[PX], zero_pos)\n",
    "        py = tf.where(non_zero, (py - self.means[PY]) / self.stddevs[PY], zero_pos)\n",
    "        pz = tf.where(non_zero, (pz - self.means[PZ]) / self.stddevs[PZ], zero_pos)\n",
    "\n",
    "        # we anyway need to flatten the vectors, so just concatenate components\n",
    "        features = tf.concat((e, px, py, pz), axis=-1)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Define the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, units=(128, 128, 128), activation=\"tanh\", dropout_rate=0., fs_args=None):\n",
    "    # track weights for later use\n",
    "    weights = []\n",
    "    \n",
    "    # input layer\n",
    "    x = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # feature scaling\n",
    "    if not fs_args:\n",
    "        fs_args = (tf.constant(4 * [0.]), tf.constant(4 * [1.]))\n",
    "    a = FeatureScaling(*fs_args)(x)\n",
    "\n",
    "    # add layers programatically\n",
    "    for n in units:\n",
    "        # build the layer\n",
    "        layer = tf.keras.layers.Dense(n, use_bias=True, activation=activation)\n",
    "        a = layer(a)\n",
    "\n",
    "        # store the weight matrix for later use\n",
    "        weights.append(layer.kernel)\n",
    "\n",
    "        # add random unit dropout\n",
    "        if dropout_rate:\n",
    "            a = tf.keras.layers.Dropout(dropout_rate)(a)\n",
    "\n",
    "    # add the softmax layer\n",
    "    y = tf.keras.layers.Dense(2, use_bias=True, activation=\"softmax\")(a)\n",
    "    \n",
    "    # build the model\n",
    "    model = tf.keras.Model(inputs=x, outputs=y, name=\"toptagging_custom\")\n",
    "\n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model, regularization_weights = create_model((120, 4), fs_args=(means_train, stddevs_train))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross entropy\n",
    "\n",
    "The last ingredient before running the training loop is the definition of the loss function. Since we only use Keras for the model building process above, we are free to use anything we want!\n",
    "\n",
    "The main component of the loss is - as above - the binary cross entropy (CE) loss, which is a common choice for classification problems that use a softmax activation in the last layer. Although many variations of CE exist (e.g. the group of *focal* losses), we stick with this simple yet powerful formula,\n",
    "$$\n",
    "\\begin{align}\n",
    "L_\\text{CE}(y, y_t) = -y_t \\cdot \\log(y)\n",
    "\\end{align}\n",
    "$$\n",
    "where $y$ is the NN prediction and $y_t$ is the ground truth.\n",
    "\n",
    "We could have also used the Keras implementation which, in combination with an *unactivated* output layer, takes a shortcut around applying exponential functions in the output layer and building logarithms again in the loss. However, we are here to learn so we do this on our own üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "#### L2 regularization\n",
    "\n",
    "The second term in our loss function does not compare predicted and expected values, but only considers the values of all weights used in the model and provides a bad feedback in case these variables obtain rather high values. To understand why high variable values are discouraged in typical NN applications, you can imagine a simple fit of a 1-D function to a set of examples (see image below).\n",
    "\n",
    "<center><img src=\"assets/nn_capacity.png\" width=\"60%\"/></center>\n",
    "\n",
    "In case a network has too few parameters (case 1), its capacity is insufficient to describe the examples with good accuracy (*underfitting*). A network with appropriate capacity (case 2) describes the data in all parts of the phase space $x$. However, guessing the correct capacity that applies equally to all parts of the usually high-dimensional phase space is not always possible.\n",
    "\n",
    "Therefore, the scenario of *overfitting* becomes relevant (case 3). In some parts of the phase space (here, for low x values), the prediction of the network fluctuates significantly to explain each example. This is realized through large values of the paramters of the underlying fit model. The same observation holds for higher-dimensional fits and hence, also for neural networks.\n",
    "\n",
    "For this reason, we introduce the $L_2$ regularization loss, which simply sums up the squares (thus $_2$) of all traininable parameters. Before adding this term to the CE loss defined above, we scale it by a factor $\\lambda$ (`l2_norm`) to control the overall strength of the $L_2$ regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the losses\n",
    "def create_losses(weights, l2_norm=0.001):\n",
    "    # cross entropy\n",
    "    @tf.function\n",
    "    def loss_ce_fn(labels, predictions):\n",
    "        # ensure proper prediction values before applying log's\n",
    "        predictions = tf.clip_by_value(predictions, 1e-6, 1 - 1e-6)\n",
    "        loss_ce = tf.reduce_mean(-labels * tf.math.log(predictions))\n",
    "        return loss_ce\n",
    "\n",
    "    # l2 loss\n",
    "    @tf.function\n",
    "    def loss_l2_fn(labels, predictions):\n",
    "        # accept labels and predictions although we don't need them\n",
    "        # but this makes it easier to call all loss functions the same way\n",
    "        loss_l2 = sum(tf.reduce_sum(w**2) for w in weights)\n",
    "        \n",
    "        return l2_norm * loss_l2\n",
    "        \n",
    "    # return a dict with all loss function components\n",
    "    return {\"ce\": loss_ce_fn, \"l2\": loss_l2_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = create_losses(regularization_weights, l2_norm=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we need an optimizer object that handles the propagation of derivatives back through the network and updates all trainable weights. There are many different optimizers out there, but for now, we will stick with the [*Adam*](https://arxiv.org/abs/1412.6980) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer with a variable learning rate\n",
    "def create_optimizer(initial_learning_rate=0.005):\n",
    "    learning_rate = tf.Variable(initial_learning_rate, dtype=tf.float32, trainable=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    return optimizer, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer, learning_rate = create_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to define the training loop. Here, we use the TensorFlow `GradientTape` which tracks all executed operations and provides the partial gradients of the loss function with respect to all traininable weights, that are used to update their values as part of the backpropagation algorithm. You can learn more on the `GradientTape` and custom training loops [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataset_train, dataset_valid, model, loss_fns, optimizer, learning_rate,\n",
    "                  max_steps=10000, log_every=1, validate_every=100): \n",
    "    # store the best model, identified by the best validation accuracy\n",
    "    best_model = None\n",
    "\n",
    "    # metrics to update during training\n",
    "    metrics = dict(\n",
    "        step=0, step_val=0,\n",
    "        acc_train=0., acc_valid=0., acc_valid_best=0.,\n",
    "        auc_train=0., auc_valid=0., auc_valid_best=0.,\n",
    "    )\n",
    "    for name in loss_fns:\n",
    "        for kind in [\"train\", \"valid\"]:\n",
    "            metrics[f\"loss_{name}_{kind}\"] = 0.\n",
    "    \n",
    "    # progress bar format\n",
    "    fmt = [\"{percentage:3.0f}% {bar} Step: {pfx[0][step]}/{total}, Validations: {pfx[0][step_val]}\"]\n",
    "    for name in loss_fns:\n",
    "        fmt.append(f\"Loss '{name}': {{pfx[0][loss_{name}_train]:.4f}} | {{pfx[0][loss_{name}_valid]:.4f}}\")\n",
    "    fmt.append(\"Accuracy: {pfx[0][acc_train]:.4f} | {pfx[0][acc_valid]:.4f} | {pfx[0][acc_valid_best]:.4f}\")\n",
    "    fmt.append(\"ROC AUC: {pfx[0][auc_train]:.4f} | {pfx[0][auc_valid]:.4f} | {pfx[0][auc_valid_best]:.4f}\")\n",
    "    fmt.append(\"(loss format: 'last train | last valid', metric format: 'last train | last valid | best valid')\")\n",
    "    fmt = \" --- \".join(fmt).replace(\"pfx\", \"postfix\")\n",
    "\n",
    "    # helper to update metrics\n",
    "    def update_metrics(bar, kind, step, labels, predictions, losses):\n",
    "        # calculate accuracy and roc auc\n",
    "        acc = calculate_accuracy(labels.numpy(), predictions.numpy())\n",
    "        auc = roc_auc_score(labels[:, 1], predictions[:, 1])\n",
    "        # update bar data\n",
    "        metrics[\"step\"] = step + 1\n",
    "        metrics[f\"acc_{kind}\"] = acc\n",
    "        metrics[f\"auc_{kind}\"] = auc\n",
    "        for name, loss in losses.items():\n",
    "            metrics[f\"loss_{name}_{kind}\"] = loss\n",
    "        # validation specific\n",
    "        if kind == \"valid\":\n",
    "            metrics[\"step_val\"] += 1\n",
    "            metrics[\"acc_valid_best\"] = max(metrics[\"acc_valid_best\"], acc)\n",
    "            metrics[\"auc_valid_best\"] = max(metrics[\"auc_valid_best\"], auc)\n",
    "            # return True when this was the best validation step\n",
    "            return acc == metrics[\"acc_valid_best\"]\n",
    "    \n",
    "    # start the loop for all batches\n",
    "    with tqdm(total=max_steps, bar_format=fmt, postfix=[metrics]) as bar:\n",
    "        for step, (c_vectors, true_vectors, labels) in enumerate(dataset_train):\n",
    "            if step >= max_steps:\n",
    "                print(f\"{max_steps} steps reached, stopping training\")\n",
    "                break\n",
    "                \n",
    "            # do a train step\n",
    "            with tf.GradientTape() as tape:\n",
    "                # get predictions\n",
    "                predictions = model(c_vectors, training=True)\n",
    "                # comput all losses and combine them into the total loss\n",
    "                losses = {\n",
    "                    name: loss_fn(labels, predictions)\n",
    "                    for name, loss_fn in loss_fns.items()\n",
    "                }\n",
    "                loss = tf.add_n(list(losses.values()))\n",
    "            # get and propagate gradients\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "            # logging\n",
    "            do_log = step % log_every == 0\n",
    "            if do_log:\n",
    "                update_metrics(bar, \"train\", step, labels, predictions, losses)\n",
    "\n",
    "            # validation\n",
    "            do_validate = step % validate_every == 0\n",
    "            if do_validate:\n",
    "                c_vectors_valid, true_vectors_valid, labels_valid = next(iter(dataset_valid))\n",
    "    \n",
    "                predictions_valid = model(c_vectors_valid, training=False)\n",
    "                losses_valid = {\n",
    "                    name: loss_fn(labels_valid, predictions_valid)\n",
    "                    for name, loss_fn in loss_fns.items()\n",
    "                }\n",
    "                is_best = update_metrics(bar, \"valid\", step, labels_valid, predictions_valid, losses_valid)\n",
    "                \n",
    "                # store the best model\n",
    "                if is_best:\n",
    "                    best_model = tf.keras.models.clone_model(model)\n",
    "            \n",
    "            bar.update()\n",
    "        else:\n",
    "            log(\"dataset exhausted, stopping training\")\n",
    "\n",
    "    print(\"validation metrics of the best model:\")\n",
    "    print(f\"Accuracy: {metrics['acc_valid_best']:.4f}\")\n",
    "    print(f\"ROC AUC : {metrics['auc_valid_best']:.4f}\")\n",
    "    \n",
    "    return best_model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, metrics = training_loop(\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    model,\n",
    "    loss_fns,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    max_steps=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = dataset_train._orig_data[2]\n",
    "labels_valid = dataset_valid._orig_data[2]\n",
    "\n",
    "predictions_train = best_model.predict(dataset_train._orig_data[0])\n",
    "predictions_valid = best_model.predict(dataset_valid._orig_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = calculate_accuracy(labels_train, predictions_train)\n",
    "acc_valid = calculate_accuracy(labels_valid, predictions_valid)\n",
    "\n",
    "print(f\"train accuracy: {acc_train:.4f}\")\n",
    "print(f\"valid accuracy: {acc_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lessons learned\n",
    "\n",
    "- We learned about eager execution and graphs.\n",
    "- We know what `tf.function`'s are and how they create and cache graphs by the means of signature tracing.\n",
    "- Feature scaling and the separation of numerical domains between network and physics application were motivated.\n",
    "- We built a data pipeline using TensorFlow datasets.\n",
    "- We created and used a custom training loop using the GradientTape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **How the results can be further improved?**\n",
    "\n",
    "What are some drawbacks ...\n",
    "\n",
    "- We only loaded a fraction of the input data.\n",
    "- The training only ran for 2000 steps, i.e., 2000 forward pass and back propagation calls. Given the amount of data, this is clearly not enough.\n",
    "- None of the hyper-parameters is tuned yet.\n",
    "\n",
    "Now, it's up to you to improve the training! Perhaps also try to include further concepts. Good starting points are\n",
    "\n",
    "- [Learning rate scheduling](https://keras.io/api/optimizers/learning_rate_schedules/exponential_decay)\n",
    "- [Batch normalization](https://keras.io/api/layers/normalization_layers/batch_normalization)\n",
    "- [Activations](https://keras.io/api/layers/activations/#selu-function)\n",
    "- [Focal loss](https://medium.com/visionwizard/understanding-focal-loss-a-quick-read-b914422913e7)\n",
    "- [...](https://lmgtfy.com/?q=How+to+improve+my+neural+network)\n",
    "\n",
    "\n",
    "Can you reach an accuracy of 85%? You can use the cells below which wrap all of the above settings and methods in less space.\n",
    "\n",
    "**Note**: If you experience notebook kernal interruptions or messages like `Allocation of XXXXXXXX exceeds XX% of free system memory` on the terminal, reduce the number of input files again with the `stop_file` parameter as we did above. Reasonble results can already be achieved with a subset of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "# ACTION REQUIRED\n",
    "n_constituents = ...\n",
    "batch_size = ...\n",
    "l2_norm = ...\n",
    "initial_learning_rate = ...\n",
    "units = ...\n",
    "activation = ...\n",
    "dropout_rate = ...\n",
    "n_train_files = ...  # set this to a value that works with your RAM\n",
    "n_valid_files = ...  # set this to a value that works with your RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data\n",
    "dataset_train, means_train, stddevs_train = create_dataset(\n",
    "    \"train\",\n",
    "    shuffle=True,\n",
    "    repeat=-1,\n",
    "    batch_size=batch_size,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_train_files,\n",
    ")\n",
    "dataset_valid, _, _ = create_dataset(\n",
    "    \"valid\",\n",
    "    batch_size=-1,\n",
    "    n_constituents=n_constituents,\n",
    "    stop_file=n_valid_files,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model, regularization_weights = create_model(\n",
    "    (n_constituents, 4),\n",
    "    units=units,\n",
    "    activation=activation,\n",
    "    dropout_rate=dropout_rate,\n",
    "    fs_args=(means_train, stddevs_train),\n",
    ")\n",
    "loss_fns = create_losses(regularization_weights, l2_norm)\n",
    "optimizer, learning_rate = create_optimizer(initial_learning_rate)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and train\n",
    "best_model, metrics = training_loop(\n",
    "    dataset_train,\n",
    "    dataset_valid,\n",
    "    model,\n",
    "    loss_fns,\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    max_steps=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create the ROC and output plots we defined above to study the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = dataset_train._orig_data[2]\n",
    "labels_valid = dataset_valid._orig_data[2]\n",
    "\n",
    "predictions_train = best_model.predict(dataset_train._orig_data[0])\n",
    "predictions_valid = best_model.predict(dataset_valid._orig_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(\n",
    "    (labels_train, labels_valid),\n",
    "    (predictions_train, predictions_valid),\n",
    "    names=(\"train\", \"valid\"),\n",
    ").show()\n",
    "\n",
    "plot_hist(\n",
    "    (predictions_valid[labels_valid[:, 1] == 0][:, 1], predictions_valid[labels_valid[:, 1] == 1][:, 1]),\n",
    "    names=(\"Light jets\", \"Top jets\"),\n",
    "    xlabel=\"Output distribution\",\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = calculate_accuracy(labels_train, predictions_train)\n",
    "acc_valid = calculate_accuracy(labels_valid, predictions_valid)\n",
    "\n",
    "print(f\"train accuracy: {acc_train:.4f}\")\n",
    "print(f\"valid accuracy: {acc_valid:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-lpi",
   "language": "python",
   "name": "ml-lpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
