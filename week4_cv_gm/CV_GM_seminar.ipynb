{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2VLgh-ODUKQ"
   },
   "source": [
    "# **Computer Vision & Generative Models seminar**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Warm up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seminar we are going to get our hands on (the very tip of the iceberg of) CV and GM: of course, these fields are huge and many things can be tried for which we unfortunately don't have enough time. So we will go for the very basics, by, firstly, implementing the **convolution and pooling operations from scratch** to see, how they change the given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wBjau3AzDUKR"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MItztb1qDUKR",
    "outputId": "cd5bc83f-75d5-4b2f-8f42-cead8464c2e5"
   },
   "outputs": [],
   "source": [
    "!curl \"https://cds.cern.ch/images/CMS-PHO-GEN-2008-028-1/file?size=large\" --output images/cms.jpg\n",
    "image_filename = \"images/cms.jpg\"\n",
    "grayscale_image = Image.open(image_filename).resize((500, 500), Image.ANTIALIAS).convert(\"L\")\n",
    "image = np.asarray(grayscale_image).copy()\n",
    "plt.imshow(image, cmap = 'gray', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWLwNz7fDUKT"
   },
   "outputs": [],
   "source": [
    "def visualization(init_image, image):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(init_image,                #numpy array generating the image\n",
    "               cmap = 'gray',             #color map used to specify colors\n",
    "               interpolation='nearest'    #algorithm used to blend square colors; with 'nearest' colors will not be blended\n",
    "              )\n",
    "    plt.title('Initial image', y=1.02, fontsize=12)\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(image, cmap = 'gray', interpolation='nearest')\n",
    "    plt.title('Image after transformation', y=1.02, fontsize=12)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go5l6JUwDUKT"
   },
   "source": [
    "### Implement convolution! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv](images/conv.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SjI9IbeVDUKT"
   },
   "outputs": [],
   "source": [
    "# convolution for 2d matrix\n",
    "def convolution(matrix, kernel, padding=0):\n",
    "    height, width = matrix.shape\n",
    "    kernel_size, _ = kernel.shape\n",
    "    out_height = height + 1 - kernel_size + 2 * padding\n",
    "    out_width  = width  + 1 - kernel_size + 2 * padding\n",
    "    \n",
    "    out = np.zeros([out_height, out_width])\n",
    "    \n",
    "    for y in range(out_height):\n",
    "        for x in range(out_width):\n",
    "            flat_matrix = matrix[y:y + kernel_size, x:x + kernel_size].reshape(-1)\n",
    "            flat_kernel = kernel.reshape(-1)\n",
    "            out[y, x] = flat_matrix.dot(flat_kernel)\n",
    "    \n",
    "    out = np.clip(out, 0, 255)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQT8zFKJDUKT",
    "outputId": "ac70bf04-11cb-4b08-8884-410e1df972f4"
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[1, 1, 1], \n",
    "                   [1, -9, 1], \n",
    "                   [1, 1, 1]])\n",
    "conv_image = convolution(image, kernel)\n",
    "\n",
    "visualization(image, conv_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LmhUlTVDUKT"
   },
   "source": [
    "### Implement maxpooling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![conv](images/maxpool.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdRD4IXQDUKT"
   },
   "outputs": [],
   "source": [
    "# max pooling for 2d matrix\n",
    "def pooling(matrix, kernel_size, stride=1):\n",
    "    height, width = matrix.shape\n",
    "\n",
    "    out_height = (height - kernel_size) // stride + 1\n",
    "    out_width  = (width  - kernel_size) // stride + 1\n",
    "\n",
    "    out = np.zeros([out_height, out_width])\n",
    "\n",
    "    for y in range(out_height):\n",
    "        for x in range(out_width):\n",
    "            out[y, x] += np.amax(matrix[y * stride:y * stride + kernel_size,\n",
    "                                        x * stride:x * stride + kernel_size])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-C96CHwbDUKT",
    "outputId": "24238041-bfdf-44a1-b668-81911063613c"
   },
   "outputs": [],
   "source": [
    "pool_image = pooling(image, 9, 9)\n",
    "\n",
    "visualization(image, pool_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zxp8JZjDUKT"
   },
   "source": [
    "## **CV vs MNIST**\n",
    "\n",
    "_Credit:_ https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "\n",
    "OK, once we implemented ourselves the very basic CV operations on images, let's try to build something more complex: a simple convolutional neural network. For the task we will try to recognize handwritten digits using the **MNIST** dataset in [PyTorch](https://pytorch.org/tutorials/). Training a classifier on the MNIST dataset can be regarded as the *hello world* of image recognition. \n",
    "\n",
    "MNIST contains 70,000 images of handwritten digits: 60,000 for training and 10,000 for testing. The images are grayscale, 28x28 pixels, and centered to reduce preprocessing and get started quicker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unJpAPwjDUKT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scVukW1_DUKT"
   },
   "source": [
    "### Prepating dataset\n",
    "\n",
    "Before we start, let's define the hyperparameters we'll be using for the experiment. Here the number of epochs defines how many times we'll loop over the complete training dataset, while `learning_rate` and `momentum` are hyperparameters for the optimizer we'll be using later on.\n",
    "\n",
    "For repeatable experiments we have to set random seeds for anything using random number generation - this means `numpy` and `random` as well! It's also worth mentioning that cuDNN uses nondeterministic algorithms which can be disabled setting `torch.backends.cudnn.enabled = False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHJ0wXEFDUKT",
    "outputId": "76d60803-b83c-4386-d353-f41b706cb1eb"
   },
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34Z8wFecDUKU"
   },
   "source": [
    "Now we'll also need [DataLoaders](https://pytorch.org/docs/stable/data.html) for the dataset. This is where TorchVision comes into play - it let's us load the MNIST dataset in a handy way. We'll use a `batch_size` of 64 for training and size 1000 for testing on this dataset. The values `0.1307` and `0.3081` used for the `Normalize()` transformation below are the global mean and standard deviation of the MNIST dataset, we'll take them as a given here.\n",
    "\n",
    "TorchVision offers a lot of handy transformations, such as cropping or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8b2c38k_DUKU"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist_data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./mnist_data/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bISQDQkDUKU"
   },
   "source": [
    "Now let's take a look at some examples. We'll use the `test_loader` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSwY3zMnDUKU",
    "outputId": "efeddc12-a911-47d8-87bd-8c4d9c426410"
   },
   "outputs": [],
   "source": [
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aX1fEqaDUKU",
    "outputId": "aebdf45c-4869-48bb-f786-fa97e3267c65"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq1426H9DUKU"
   },
   "source": [
    "### Building the Network\n",
    "\n",
    "Now let's go ahead and build our network. We'll use two 2-D convolutional layers followed by two fully-connected (or linear) layers. As activation function we'll choose rectified linear units (ReLUs in short) and as a means of regularization we'll use two dropout layers. In PyTorch a nice way to build a network is by creating a new class for the network we wish to build. Let's import a few submodules here for more readable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4vSc6N4DUKU"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny78-3FFDUKU"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXYHZo8RDUKU"
   },
   "source": [
    "Broadly speaking we can think of the `torch.nn` layers as which contain trainable parameters while `torch.nn.functional` are purely functional. The `forward()` pass defines the way we compute our output using the given layers and functions. It would be perfectly fine to print out tensors somewhere in the forward pass for easier debugging. This comes in handy when experimenting with more complex models. Note that the forward pass could make use of e.g. a member variable or even the data itself to determine the execution path - and it can also make use of multiple arguments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv9jmQVuDUKU"
   },
   "source": [
    "Now let's initialize the network and the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXCJxi7QDUKU"
   },
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-YuESyGDUKU"
   },
   "source": [
    "### Training the Model\n",
    "\n",
    "Time to build our training loop. First we want to make sure our network is in training mode. Then we iterate over all training data once per epoch. Loading the individual batches is handled by the DataLoader. First we need to manually set the gradients to zero using `optimizer.zero_grad()` since PyTorch by default accumulates gradients. We then produce the output of our network (forward pass) and compute a negative log-likelihodd loss between the output and the ground truth label. The `backward()` call we now collect a new set of gradients which we propagate back into each of the network's parameters using `optimizer.step()`.\n",
    "\n",
    "We'll also keep track of the progress with some printouts. In order to create a nice training curve later on we also create two lists for saving training and testing losses. On the x-axis we want to display the number of training examples the network has seen during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMV4-OzuDUKU"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wozMsgNjDUKU"
   },
   "source": [
    "We'll run our test loop once before even starting the training to see what accuracy/loss we achieve just with randomly initialized network parameters. Can you guess what our accuracy might look like for this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osWo5KjEDUKU"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pAAK_uifDUKU"
   },
   "source": [
    "Now for our test loop. Here we sum up the test loss and keep track of correctly classified digits to compute the accuracy of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRSLKwfrDUKU"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bspc3Z6mDUKV"
   },
   "source": [
    "Using the context manager `no_grad()` we can avoid storing the computations done producing the output of our network in the computation graph.\n",
    "\n",
    "Time to run the training! We'll manually add a `test()` call before we loop over n_epochs to evaluate our model with randomly initialized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O3n9Uz0MDUKV",
    "outputId": "b5fbdaff-9246-4d9c-c6e0-dc6881c72c43"
   },
   "outputs": [],
   "source": [
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjaUaJhRDUKV"
   },
   "source": [
    "### Evaluating model performance\n",
    "\n",
    "And that's it. With just 3 epochs of training we already managed to achieve 97% accuracy on the test set! We started out with randomly initialized parameters and as expected only got about 10% accuracy on the test set before starting the training.\n",
    "\n",
    "Let's plot our training curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVd_JKI0DUKV",
    "outputId": "4244d70b-5dd1-4844-fd30-5759ca326a29"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H5O855NDUKV"
   },
   "source": [
    "Judging from the *training curve* it looks like we could even continue training for a few more epochs\n",
    "\n",
    "But before that let's again look at a few examples as we did earlier and compare the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bpvDoqcDUKV",
    "outputId": "64ec5261-2daa-42d2-a689-8baabb62c27d"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = network(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmffLSUNDUKV",
    "outputId": "80d9972d-50d2-4711-9f70-c40ca28c080b"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(20):\n",
    "    plt.subplot(4,5,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s818igVODUKV"
   },
   "source": [
    "## **VAE vs MNIST**\n",
    "\n",
    "_Credit:_ https://github.com/lyeoni/pytorch-mnist-VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've just learned to classify various MNIST digits, let's step into the land of generative models, and **Variational Autoencoder** in particular. Below, we build it from scratch using PyTorch now to _generate_ MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVcBF0VmDUKV"
   },
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "batch_size_train = 100\n",
    "batch_size_test = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST('./mnist_data/', train=True, download=True,\n",
    "            transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
    "\n",
    "test_dataset = datasets.MNIST('./mnist_data/', train=False, download=True,\n",
    "                transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor()]))\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThwScZ_oDUKV"
   },
   "source": [
    "### Implementing architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfoSyZZqDUKV"
   },
   "source": [
    "![vae](images/vae.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1Hhz6vfDUKV"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPju9xTnDUKV",
    "outputId": "33bf9769-67b1-4b49-df3d-b286528ae8e1"
   },
   "outputs": [],
   "source": [
    "vae = VAE(x_dim=28*28, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZXTvukPDUKW"
   },
   "source": [
    "Loss explanation: https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iY2nJWyTDUKW"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQnjBFLGDUKW"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haKGIuj1DUKW"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            recon, mu, log_var = vae(data)    \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3c4eHvXXDUKW",
    "outputId": "5994c130-76f6-4b98-e8ca-2a217a2998f6"
   },
   "outputs": [],
   "source": [
    "for epoch in range(3):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p5KJamavDUKW",
    "outputId": "2d0fa022-801d-4c53-ffba-391489ca5d13"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z = torch.randn(64, 2)\n",
    "    sample = vae.decoder(z)\n",
    "    \n",
    "images = sample.view(64, 1, 28, 28)\n",
    "    \n",
    "fig = plt.figure()\n",
    "for i in range(12):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(images[i][0], cmap='gray', interpolation='none')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SeminarCVGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-lpi",
   "language": "python",
   "name": "ml-lpi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
